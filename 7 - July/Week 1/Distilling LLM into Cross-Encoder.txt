A Systematic Investigation of Distilling Large Language Models into Cross-Encoders for Passage Re-ranking

16 Jun 2024

Abstract
    - Cross-encoders distilled from LLMs are often more effective re-rankers than cross-encoders fine-tuned on manually labeled data
    - Distilled models usually do not reach their teacher LLM's effectiveness
    - We investigate whether best practices for fine-tuning cross-encoders on manually labeled data (eg hard-negative sampling, deep sampling, listwise loss functions) can help to improve LLM ranker distillation
    - We construct and release a new distillation dataset:
        - Rank-DistiLLM
    - Cross-encoders trained on Rank-DistiLLM reach the effectiveness of LLMs while being orders of magnitude more efficient


Intro
    - Cross-encoders using pre-trained transformer-based models are among the most effective passage re-rankers
    - Require large amounts of labeled data for fine-tuning
    - LLMs require no further fine-tuning to excel in re-ranking tasks and are often more effective than cross-encoders
    - Main drawback of LLMs: computational cost
        - Need several seconds to re-rank 100 passages for a single query
        - Impractical for production search engines, but LLMs can be used to create training data for fine-tuning cross-encoders
    - Initial work showed that cross-encoders distilled from LLMs are more effective re-rankers than cross-encoders fine-tuned on manually labeled data
        - The distilled cross-encoders did not reach effectiveness of the LLMs
        - One reason: distilled models were created w/o considering best practices for fine-tuning re-rankers on manually labeled data
            - No "hard-negative" sampling was used, which requires an effective first-stage retrieval model to sample data
            - At most 30 passages per query were provided, which is not deep enough to achieve optimal effectiveness
            - No listwise losses were used, which are usually more effective than pair/pointwise losses
    - We systematically investigate the distillation of cross-encoders from LLMs
    - With our newly constructed Rank-DistiLLM dataset we analyze: 
        - The effect of the first-stage retrieval model
        - The ranking depth
        - Amount of training data on the distilled cross-encoder's effectiveness
    - We propose a listwise loss function for distillation from ranking data

    - We find that our listwise loss function yields no benefit over a pairwise loss function
    - Distilling cross-encoders w/our new dataset helps to close the effectiveness gap to LLMs
    - We achieve similar effectiveness as SoTA ranking LLMs while being orders of magnitude more efficient


Related Work
MS MARCO Fine-Tuning
    - MS MARCO is the most commonly used dataset for fine-tuning cross-encoders
    - 500k query-passage pairs
    - Most queries have a single passage labeled as relevant
    - Label sparsity implies
        1. Options for suitable loss functions are limited
            - List-wise losses produce the most effective models
            - Use a single relevant passage and a set of k heuristically sampled "non-relevant" passages to compute the loss
            - A higher k produces more effective models, with k=36 the highest reported value
            - We use memory-efficient fused-attention kernels to circumvent the memory constraints of the Transformer's self-attn mechanism and fine-tune models on up to k=100 passages
        2. "Non-relevant" passages have to be sampled heuristically
            - "Hard-negative" sampling (using an effective first-stage retrieval model to sample "non-relevant" samples) has produced the most effective models
            - Models fine-tuned on negative samples from ColBERT are more effective than those fine-tuned on negative samples from BM25
            - MS MARCO contains passages that are more relevant than the labeled passage for a substantial number of queries, leading to noisy training data
    - Distillation from LLMs
        - To obtain less noisy data, fine-tune a cross-encoder on the rankings generated by an LLM applied in a zero-shot manner
        - Models fine-tuned on their released dataset are more effective in low-data settings and out-of-domain re-ranking than those fine-tuned on MS MARCO
        - Baldelli released a smaller dataset using a variety of first-stage retrieval models
        - Cross-encoders are even more effective when fine-tuned on this dataset, but an effectiveness gap between the cross-encoder and LLMs remains
        - We investigate if thie gap can be closed by applying the insights from fine-tuning on MS MARCO to LLM distillation


Cross-Encoders
    - Process the query and passage simultaneously using a pre-trained transformer-based encoder model
    - Given sequences of query tokens q and passage tokens p, the encoder's input sequence is [CLS] q [SEP] p [SEP]
    - The model outputs contextualized embedding vectors for every token
    - Using learnable weights and biases, it then applies a linear layer to the [CLS] token's contextualized embedding to compute the relevance score sp = W * e[CLS] + b
Fine-Tuning on MS MARCO
    - Loss
        - When fine-tuning cross-enc on data sampled from MS MARCO, previous work obtains the most effective models by using listwise sm cross-entropy or localized contrastive estimation loss (LCE)
        - Both are equivalent when only a single relevant passage is available
    - Data
        - For highest effectiveness, P (set of passages) should be as large as possible, and the best available first-stage retrieval model should retrieve the other passsages
        - We use ColBERTv2 to retrieve the top 200 passages for all MS MARCO training queries
        - We then randomly sample up to 99 hard negatives
Fine-Tuning on LLM Distillation Data
    - Loss
        - Instead of a set of passages P, LLM distillation data consists of a ranked list of passages R = [p1,...,pn] for a query q
        - Previous work uses RankNet, a pairwise loss function for distillation fine-tuning
    - For fine-tuning on MS MARCO, listwise loss functions are more effective than pairwise loss
    - We propose a new loss function
        - Approx Discounted Rank MSE computes the MSE between a passage's actual and approximated rank
    - Data
        - Only two datasets for distilling cross-encoders from LLMs have been released
        - RankGPT: top 20 passages retrieved by BM25 and re-ranked by RankGPT-3.5 for 100k queries from MS MARCO
        - TWOLAR: top 30 passages by three different retrieval models and re-ranked by RankGPT-3.5 for 20k queries from MS MARCO
        - Cross-enc fine-tuned on the TWOLAR dataset are more effective than when fine-tuned on the RankGPT dataset
        - Still, whether the improved first-stage retrieval models, deeper rankings, or both in combination lead to better effectiveness remains unclear
    - We create Rank-DistiLLM to systematically investigate the effect of the first-stage retrieval model and the rank depth on a cross-encoder's down-stream effectiveness
    - We retrieve the top 100 passages using BM25 and colBERTv2 for 10k randomly sampled queries from the MS MARCO training set
    - We use RankZephyr to re-rank them


Evaluation
Labeled Data vs LLM Distillation
    - 