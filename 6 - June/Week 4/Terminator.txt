HyperZ·Z·W Operator Connects Slow-Fast Networks for Full Context Interaction

31 Jan 2024

Abstract
    - SA uses large implicit weight matrices
        - Dot product based activations
        - Very few trainable parameters to enable long seq modeling
    - We investigate the possibility of discarding residual learning by employing large implicit kernels to achieve full context interaction at each layer
    - We introduce coordinate-based implicit MLPs as a slow network to generate hyper-kernels
    - Fast conv network
    - To get context-varying weights, we propose an operator that connects hyper-kernels and hidden activations thru simple elementwise multiplication, followed by conv of Z using the context-dependent W
    - We propose a novel Terminator architecture that integrates hyper-kernels to produce multi-branch hidden representations for enhancing feature extraction capability
    - A bottleneck layer compresses the concatenated channels, so only valuable information propagates to subsequent layers
    

Method
Key Properties
    - No residual learning
        - Residual learning has been employed to address performance degradation in very deep networks
        - We contend: residual connects compensate for the limited representation learning capacity of individual layers
        - To overcome these limitations, we incorporate large implicit convolution kernels to construct a multi-branch network
        - Full context interaction at each layer
    - No dot-product attention
        - Quadratic increase in computational cost based on num pixels necessitates the division of images into patches to facilitate ViT
        - The conversion of patch embeddings into fixed-length representations limits the inclusion of local context and hinders the estimation of attn scores for individual pixels
        - HyperZZW operator enables interaction between generated hyper-kernels and image pixels through a simple elementwise multiplication
        - This facilitates the acquisition of precise pixel-level scores
    - No intermediate pooling layer
    - No normalization
    - Slow-Fast networks
        - Slow network: coordinate-based implicit MLPs
        - Generates large convolution kernels (hyper-kernels) for a fast network
        - The slow network has minimal trainable parameters but can generate large hyper-kernels to achieve full context interaction at each layer
        - Notably, the weights of the slow network remain fixed after training and do not directly interact with the context
        - The fast network consists of two components:
            1. Channel mixers and bottleneck layer, interacts with feature maps
            2. Context-dependent fast weights generated by HyperZZW operator, enables convolution and dot product operations
        - Fast weights can adapt in real0time during testing based on input (?)
Generation of Hyper-Kernels
    - We generate two distinct hyper-kernels w/slow networks to construct multi-branch networks
    - The first kernel: Global hyper-kernel has same size as input
    - Second kernel: Local hyper-kernel has small kernel size k
    - Combine the hyper-kernels to extract features while preserving input's fine details
    - We elaborate on generation process of Kg
    - Global hyper-kernel Kg is generated w/coordinate-based implicit MLPs
        - Multiplicative filter networks
HyperZZW Operator
    - Our proposed HyperZZW operator enables the generated global and local hyper-kernels to interact with the context, resulting in context-dependent fast weights
    - Utilization includes:
        1. Hidden act layer Z and hyper-kernel W are multipied element-wise to obtain context-varying weights `W`
        2. `W` can be used to perform dot product or convolution operations on Z for feature extraction
    - HyperZZW operator achieves context-dependent fast weights w/linear time and space complexity
    - Global HyperZZW, Local HyperZZW
Slow-Fast Neural Encoding Block
    - SFNE simultaneously achieves full context interaction in channel and spatial dimensions
    - Figure 3 presents a nine-branch architecture, composed of 3 branches from global HZZW, one branch from SiGlu, one from middle layer, and one from hyper interaction
    - Global branches share a common hyper-kernel
    - 