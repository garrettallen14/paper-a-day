Soaring from 4K to 400K: Extending LLMâ€™s Context
with Activation Beacon

7 Jan 2024

Abstract
- Activation Beacon: condenses LLM's raw activations into more compact forms
- Can perceive a much longer context w/a limited context window
- Fully preserves original capability while extending new capability on processing longer contexts

- Activation Beacon is learned by the auto-regression task conditioned on a mixture of beacons with diversified condensing ratios
- Can extend Llama-2-7B context length by 100x (4k to 400k)
- Code available at the BGE repository


Introduction
- LLM's raw activations (intermediate outputs produced as they process text) are informationally redundant
- They contain more information than necessary for understanding / generating text
- Condense these activations into a more compact form => LLM can cover a broader context with a smaller context window

- In the work, LLM's raw activations are condensed by special tokens called beacons.
- For one interval of length L, a team of k beacons are deployed (L>>k) to obtain condensing ratio alpha = L/k
- Beacons are param efficient because they mostly rely on the LLMs original params
- Beacons are learned in a post-hoc manner on top of a fixed LLM.
- Can work as a plug-and-play component, introducing extended contextual information

- Condensed activations are streamingly processed by sliding windows
- [bcn1, ... bcnm, xm+1, ..., xn] where bnci represents the beacon generated by the previous windows and x is an ordinary token in the current window
- Size of the sliding window is upper-bounded by the max context length of the LLM
- Long seq data can be processed window-by-window instead of simultaneously

- Activation Beacon is learned through the auto-regression task:
    - In each sliding window, the generation likelihood of one ordinary token xi is maximized based on the beacons and its preceding ordinary tokens:
    - max p(xi | bcn1, ..., bcnm, xm+1, ..., xi-1)
- An aggressive extension of the context length calls for a large condensing ratio, while a moderate extension just needs a small condensing ratio, we perform random sampling of alpha during the streaming process.
