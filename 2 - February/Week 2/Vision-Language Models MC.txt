Vision-Language Models Provide Promptable Representations for
Reinforcement Learning

5 Feb 2024

Abstract
    - RL Agents typically have to learn behaviors from scratch
    - Propose an approach that uses the world knowledge encoded in vision-language models (VLMs)
    - Initialize policies with VLMs by using them as promptable representation

Introduction
    - We aim for agents to interpret tasks in terms of concepts that can be reasoned about with relevant prior knowledge grounded in previously-learned Representations
    - Doing so requires a condensed source of vast amounts of general-purpose world knowledge, captured in a form that allows us to specifically index into and access task-relevant information
    - We need representation that are contextual, such that agents can use a concise task context to draw out relevant background knowledge, abstractions, and grounded geatures that aid it in acquiring a new behavior

    - One approach: integrating RL agents with the prior knowledge and reasoning abilities of pre-trained foundation models
    - (V)LMs can be used to reason about goals to produce executable plans, or as encoders of useful information (like instructions or feedback)
    - Limitations: actions generated by LMs are often not appropriately grounded, unless the tasks and scenes are amenable to being expressed or captioned in language
    
    - We introduce Promptable Representations for RL: a flexible framework for guiding VLMs to produce semantic features which
        i. Integrate observations with prior task knowledge
        ii. are grounded into actions via RL
    - We ask a VLM questions about observations that are related to the given control task, encouraging it to attend to task-relevant features in the image

    - We introduce the first approach for initializing RL policies with generative VLM representations


Promptable Representations for RL
    - Goal: supplement RL w/the task-relevant information extracted from VLMs containing general-purpose knowledge
    