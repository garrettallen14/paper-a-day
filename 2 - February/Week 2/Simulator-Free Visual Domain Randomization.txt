Simulator-Free Visual Domain Randomization via Video Games

2 Feb 2024

Abstract
    - BehAVE: a video understanding framework
    - Leverages the plethora of existing commercial video games for domain randomization, w/o requiring access to the simulation engines
        1. The rich visual diversity of video games acts as a source of randomization
        2. Player behavior -- represented semantically via textual descriptions of actions -- guides the alignment of videos with similar content
    - We test BehAVE on 25 FPS games
    - Report its robustness for domain randomization
    - BehAVE successfully aligns player behavioral patterns and is able to zero-shot transfer them to multiple unseen FPS games when trained on just one FPS game
    - Improves zero-shot transferability of foundation models to unseen FPS games (up to 22%) even when trained on a game of a different genre (Minecraft)
    

Introduction
    - Computer Vision needs to be adaptable across visually distinct domains to minimize efforts needed for seamless transferability
    - One promising avenue is through domain randomization: a simple technique that improves the robustness of a CV model by training it on visuals derived from randomizing rendering parameters of a simulation engine
    - Building accurate (world-realistic) and large-scale simulators is a formidable challenge which requires significant time, expertise and effort.
    - Identifying and randomizing relevant simulation parameters adds further to the complexity
    - To reduce the reliance of CV on costly simulations, we introduce a novel approach to domain randomization:
        - Leveraging the rich visual diversity inherent in video games
    - We introduce a learning framework that distinguishes itself by not relying on game engine access for the randomization process
    - This way, CV can be trained and tested on videos from commercial-grade video game titles
        - BehAVE:
            - Harnesses the unique characteristic of gameplay videos opposed to any other videos available
            - Gameplay footage is generated by a latent generative factor: sequential player actions (ie, controller inputs) which control on-screen animations characterized as player behavior
            - Using player actions, BehAVE is able to align video encodings across visually diverse games (ie, different domains)
            - Employs semantic action encoding:
                - infuses semantic information about behavior through textual descriptions of actions, encoded by pre-trained text encoders
            - As a result, the text encodings of player actions, expressing a player's behavior, steer the behavioral alignment of video encodings
    
    - We train BehAVE's alignment module, implemented on top of foundation video encoders across a diverse array of games from FPS genre
        - Namely, on SMG-25 dataset
    - BeHAVE systematically restructures the video encoder's representation space and groups closely together the encodings of videos that showcase similar player behavior
    - BeHAVE is able to uncover similar behavioral patterns -- despite visual distinctions, such as variations in game style or aesthetics -- across unseen games of the SMG-25 dataset
    
    - To assess transferability, we examine transfer capacity of the behavior-aligned video encodings in accurately classifying player behavior across various FPS games, while solely being trained on CS:GO
    - Our findings indicate higher transferability when learning to classify behavior from our aligned representation space as compared to without
    - Showcases up to 22% higher classification accuracies across the different behavior categories tested

    - Key contributions:
        1. Introduce the BehAVE framework for domain randomization via commerical video games
            - Aligns video encodings of similar player behavior in video games using player actions as a supervision label, and operates without requiring access to game engine
        2. Propose Semantic Action Encoding
            - Representing player actions as textual descriptions processed through a pretrained text encoder which serves the purpose of injecting semantic information to the alignment training
        3. We offer extensive experimental results of BehAVE on our newly introduced SMG-25 dataset


Background
    - Despite Foundation Model's impressive out-of-the-box performance, FMs are limited in ability from one domain to another
    - Domain randomization needed to train transferable vision models by introducing variability (ie, injecting domain gap) during learning
        - Achieved through the randomization of rendering in a simulator generating training data
    - Current work uses LMs to encode player actions
        - BehAVE then uses these action encodings for alignment with another modality, gameplay
    - Rely on multimodal alignment frameworks


BehAVE Framework
    - Behavior Alignment of Video Game Encodings
    - Algorithm 1: Behavior-Alignment Training with BehAVE:
        INPUT: games dataset D, semantic action mapper m, pretrained text encoder h, pretrained video encoder f, trainable alignment projector p
            for (video V, actions A) in D:
                compute video encoding z(video) = f(V)
                project to aligned encoding z(align) = p(z(video))
                convert to behavior text caption Acaption = m(A)
                calculate loss Lcos = 1 - cosine(z(align), z(caption))
                back-prop Lcos
                Update projector network parameters pTheta
            end
        OUTPUT: Trained alignment projector p

Games Dataset
    - Meticulous preparation of a dataset adhering to a specific structure that accommodates semantically similar visual content across diverse visual styles
    - Game selection for domain randomization:
        - let G represent a game, frame renderer g, and G in C (the family of games of a certain game genre category)
        - There are certain game style parameters associated with the textures and color palette of game objects
        - Game physics
        - In this context, we observer eG in X where X represents the diverse global game-design space
        - This introduces implicit randomization into our framework
        - We also characterize all game-state-specific params of the renderer, including player's spatial coordinates, health or ammunition status, and camera perspective
    - Synchronized gameplay recording:
        - At each timestep t, we obtain two synchronzed information streams: visuals and actions
        - PLayer inputs / actions are selected from the shared action space of the game genre C
            - Represented by a set of unique keypressess
            - Visuals recorded in RGB frames in Rhxwx3, (height, width)
            - Visuals are a dynamic sequences of frames that arise from the interactions between the player and the game, as follows: F[t+1] = g(Ft, At, paramsCt | paramsG)
            - Thus, the video frames are generated sequentially by the game renderer g processing the game content and player action information at every timestep
    - Data Pre-processing:
        - We collect data at the timestep level
        - Our framework operates on videos for Identifying behavior
        - so, We aggregate data over consecutive timesteps, forming a window of length T to obtain video sequences V=(F1,...,FT) and action sequences A=(A1,...,AT)
        - Consequently, for each game, the synchronized gameplay-actions dataset is denoted as D^G = {(Vi,Ai)} for i in I, where |D^G| = I.
        - We construct the overall dataset D = UnionG of D^G

Encoding the Modalities
    - Video Encoding and Alignment
        - We harness the capabilities of a pre-trained video foundation model
        - f denotes the pretrained backbone of a video encoder
        - z(video) = f(V) is the latent representation of the backbone model
        - Then, we employ a trainable MLP model p that acts as an alignment projector and operates on top of the video encoder
        - z(align) = p(z(video))

Semantic Action Encoding
    - Each video is associated with a sequence of binary actions A, indicating the presence of absence of each key-press action at every timestep.
    - However, binary labels for actions offer limited insights into the inter-relationships among various sub-actions
        - Move left, move right, shoot gun, aim gun... all are equidistant to one another...
    - We propose to equip BehAVE with a hand-crafted semantic action mapper function m
        - Injects semantic information into the action encodings via text
        - Maps the binary sequence of keypresses to a behavior text caption A^caption = m(A) that describes the semantic behavior associated with that action
        - We then use a pretrained text foundation model in the form of a text encoder z^caption = h(A^caption)
        - Pre-trained encoders will be able to better capture the inter-relationships among the joint distribution of actions that are otherwise difficult to represent with binary action encodings.

Alignment Training
    - Once we obtain the encodings of videos and actions, we initiate the training phase of the framework
    - We observe different video sequences exhibiting similar behavior across different games
    - To align the representation space of the video encoder to match the text encoding of the behavior, we choose to train our projector head as attached to the video encoder
    - We use a loss Lcos based on the cosine similarity between the video projector encoding
        Lcos(z^align, z^caption) = 1 - (z^align * z^caption) / (||zalign||||zcaption||)

Experiments
    a. Behavior alignment: we perform alignment training
        - 
    b. Behavior classification