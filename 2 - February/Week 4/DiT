Scalable Diffusion Models with Transformers

2 Mar 2023

Abstract
	- Explore a new class of diffusion models based on transformer architecture
	- Train latent diffusion models of images, replacing the commonly-used U-Net backbone w/ a transformer
	- We analyze the scalability of DiTs through the lens of forward pass complexity in Gflops
	- DiTs w/higher Gflops--through increased transformer depth/width or input tokens--consistently have lower FID
	- Our DiT-XL/2 models outperform all prior diffusion models on class-conditional ImageNet 512x512 and 256x256 benchmarks
	

Introduction
	- By constructing and benchmarking the DiT design space under the Latent Diffusion Models (LDMs) framework, where diffusion models are trained within a VAE's latent space, we can successfully replace the U-Net backbone w a transformer
	- We further show that DiTs are scalable architectures for diffusion models: there is a strong correlation between network complexity...
	

Diffusion Transformers
Diffusion Formulation:
	- Gaussian diffusion models assume a fwd noising process which gradually applies noise to real data x0: q(xt|x0) = N(xt; ...)
	- By applying the reparameterization trick, we can sample xt = ...
	
	- Diffusion models are trained to learn the reverse process that inverts forward process corruptions: ptheta(xt-1|xt)
	- Where neural networks are used to predict the statistics of ptheta
	- The reverse process model is trained with the ELBO
	
	- The model can be trained using simple MSE between predicted noise and ground truth sampled Gaussian noise.
	
	- Follow Nichol and Dhariwal's approach:
		- Train etheta with Lsimple (random noise)
		- Train sigmatheta with full Loss
		- Once ptheta trained, new images can be sampled by:
		- Initialize xtmax ~ N(0, I)
		- Sample xt-1 ~ ptheta(xt-1|xt) via reparameterization trick
		
Classifier-free Guidance
	- Conditional diffusion models take extra information as input, such as a class label c.
	- In this case, the reverse process becomes ptheta(xt-1|xt,c)
		- Where etheta and sigma theta are conditioned on c
	- In this setting, classifier-free guidance can be used to encourage sampling to find x st log p(c|x) is high...
	- By Bayes Rule, log p(c|x) is proportional to log p(x|c) - log p(x)
	- Therefore, derivative of log p(c|x) is proportional to derivative of log p(x|c) - log p(x)
	- By interpreting the output of diffusion models as the score function, the DDPM sampling procedure can be guided to sample x with high p(x|c) by:
		- e`theta(xt, c) = etheta(xt,phi) + s * ...
	- Classifier-free guidance is widely-known to yield significantly improved samples over generic sampling techniques, trend holds for DiT
	
Latent Diffusion Models
	- Training diffusion models directly in high-res pixel space can be computationally prohibitive
	- LDMs tackle this with:
		1. learn an autoencoder that compress images into smaller spatial representations with a learned encoder E
		2. train a diffusion model of images x (E is frozen). New images can then be generated by sampling a representation z from the diffusion model and subsequently decoding it to an image with the learned decoder x = D(z)
	- In our exploration we use off the shelf conv VAEs and transformer-based DDPMs
	
Diffusion Transformer Design Space
	- DiT is based on the ViT architecture
Patchify
	- The input to DiT is a spatial representation z (for 256x256x3 images, z has shape 32x32x4)
	- The first layer of DiT is "patchify" which converts the spatial input into a sequence of T tokens, each of dim d, by linearly embedding each patch in the input.
	- Following patchify, we apply standard ViT frequency-based positional embeddings (sine-cosine version) to all input tokens.
	- The number of tokens T created by patchify is determined by the patch size hyperparam p
	- Halving p will quadruple T => at LEAST quadruple total transformer Gflops
	- Changing p has no meaningful impact on downstream param counts
DiT block design
	- Following patchify, the input tokens are processed by a sequence of transformer blocks.
	- In addition to noised image inputs, diffusion models sometimes process additional conditional information such as noise timesteps t, class labels c, natural language, etc.
	- We explore four variants of transformer blocks that process conditional inputs differently.
	- The designs introduce small, but important modifications to the standard ViT block design.
	- In-context conditioning:
		- We simply append the vector embeddings of t and c as two additional tokens in the input sequence, treating them no differently from the image tokens. This is similar to cls tokens in ViTs and allows us to use standard ViT blocks w/o modification
		- After the final Block, we remove the conditioning tokens from the sequence
		- This approach introduces negligible new Gflops to the model
	- Cross-attention block
		- We concatenate the embeddings of t and c into a length-two sequence, separate from the image token sequence, the transformer block is modified to include an additional multi-head cross-attention layer following the multi-head SA block
		- Cross-attention adds the most Gflops to the model, roughly 15% overhead
	- Adaptive layer norm (adaLN) block
		- Following the widespread usage of adaptive normalization layers in GANs and diffusion models with U-Net backbones, we explore replacing standard layer norm layers in Transformer blocks with adaptive layer norm (adaLN)
		- Rather than learn dimensionwise scale and shift parameters, we regress them from the sum of the embedding vectors of t and c. Of the three block designs we explore, adaLN adds the least Gflops and is thus the most compute-efficient
		- It is also the only conditioning mechanism that is restricted to apply the same function to all tokens.
	- adaLN-Zero block
		- Prior work on ResNets has found that initializing each residual block as the identity function is beneficial
		
Model Size:
	- We apply a sequence of N DiT blocks, each operating at the hidden dim size d.
	- Following ViT, we use std transformer configs that jointly scale N, d and attention heads
