Denoising Diffusion Probabilistic Models

Abstract
    - We present high quality image synthesis results using diffusion prob. Models


Introduction
    - Diffusion models are parameterized Markov chains trained using variational inference to produce samples matching the data after finite time.
    - Transitions of the chain are learned to reverse a diffusion process, which is a Markov chain that gradually adds noise to the data in the opposite direction of sampling until signal is destroyed
    - When the diffusion consists of small amounts of Gaussian noise, it is sufficient to set the sampling chain transitions to conditional Gaussians too, allowing for a particularly simple neural network parameterization

    - Diff models are straightforward to define and efficient to train.
    - There has been no demonstration they are capable
    - We show a certain parameterization of diffusion models reveals an equivalence with denoising score matching over multiple noise levels during training

    - Despite their sample quality, our models do not have competitive log likelihoods compared to other likelihood-based models
    - We find that the majority of our models' lossless codelengths are consumed to describe imperceptible image details.


Background
    - 