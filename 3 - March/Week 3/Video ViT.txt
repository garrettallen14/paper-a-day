ViViT: A Video Vision Transformer

1 Nov 2021

Abstract
    - We present pure-transformer based models for video classification
    - Extract spatio-temporal tokens from input video
    - These tokens are then encoded by a series of transformer layers
    - To handle the long sequences of tokens encountered, we propose several, efficient variants which factorise the spatial and temporal dimensions


Introduction
    - We propose pure-transformer models for video classification


Video Vision Transformers
Overview of ViTs
    - Adapt the transformer architecture to process 2D images with minimal changes
    - ViT extracts N non-overlapping image patches, performs a linear projection, then rasterises them into 1D tokens
    - The sequence of tokens input to the followin transformer encoder is z = ..., where the projection by E is equivalent to a 2D convolution

    - The tokens are then passed through an encoder consisting of a sequence of L transformer layers
    - Each layer l comprises of MH S-A, layer norm and MLP blocks

    - The transformer is a flexible architecture that can operate on any sequence of input tokens, so we describe strategies for tokenising videos next

Embedding video clips
    - We consider two simple methods for mapping a video V in R timexheightxwidthxchannel to a sequence of tokens z` in Rntxnhxnhxd
    - We then add the positional embedding and reshape into RNxd to obtain z, the input to the transformer
Uniform frame sampling
    - A straightforward method of tokenising the input video:
        1. Uniformly sample nt frames from the input video clip
        2. Embed each 2d frame independently using the same method as ViT
        3. Concatenate all tokens together
        - If nh * nw non-overlapping image patches are extracted from each frame, then nt*nh*nw tokens will be forwarded through the transformer encoder
        - This process may be seen as simply constructing a large 2D image to be tokenised following ViT
Tubelet embedding
    - An alternative method is to extract non-overlapping, spatio-temporal "tubes" from the input volume, and to linearly project this to Rd
    - This method is an extension of ViT's embedding to 3D, and corresponds to a 3D convolution
    - For a tubelet of dimension txhxw, nt = T/t, nh = H/h, nw = W/w tokens are extracted
    - Smaller tubelet dimensions thus result in more tokens which increases the computation
    - This method fuses spatio-temporal information during tokenisation, in contrast to "Uniform frame sampling" where temporal information from different frames is fused by the transformer
Transformer Models for Video
Model 1: Spatio-temporal attention
    - This model simply forwards all spatio-temporal tokens extracted from the video, z0, through the transformer encoder
    - We note that this has also been explored concurrently in the "Joint Space-Time" model
    - The transformer models long-range interactions across the video from the first layer
    - As it models all pairwise interactions, MSA has quadratic complexity wrt the number of tokens
    - The number of tokens increases linearly with the number of input frames, and motivates development of more efficient architectures next
Model 2: Factorised encoder
    - Consists of two separate transformer encoders
    - First: spatial encoder
        - Only models interactions between tokens extracted from the same temporal index
        - A representation for each temporal index hi in Rd is obtained after Ls layers
        - This is the encoded classification token if it was prepended to the input, or a global average pooling from the tokens 
        - ...
        - This models requires fewer FLOPs
Model 3: Factorised self-attention
    - This model contains the same number of transformer layers as Model 1.

Initialization by leveraging pretrained models
    - ViT has been shown to only be effective when trained on large-scale datasets
Positional Embeddings:
    - A positional embedding p is added to each input token.
    - However, our video models have nt times more tokens than pretrained image model
    - As a result, we initialise the positional embeddings by "repeating" them temporally from Rnw*nhxd to ...
    - Therefore, at initialization, all tokens with the same spatial index have the same embedding which is then fine-tuned
    