Scaling Vision Transformers

20 Jun 2022

Abstract
    - ViTs are great
    - Scale is a primary ingredient in attaining excellent results
    - Understanding scaling properties is a key to designing future generations effectively
    - We scale ViT models and data both up and down and characterize the relationships between error rate, data and compute
    - We refine the architecture and training of ViT, reducingmemory consumption and increasing accuracy of resulting models
    - We successfully train a ViT model with two billion parameters, attaining a new SOTA on ImageNet of 90.45% top-1 accuracy
    

Introduction
    - We experiment with models ranging from 5m to 2b parameters
    - Datasets ranging from 1m to 3b training images
    - Compute budgets ranging from >1 TPUv3 core-day to beyond 10k core-days
    
    - We create an improved large-scale training recipe
    - Investigate training hyper-parameters and discover subtle choices
    - Very strong L2 regularization, applied to the final linear prediction layer only, results in a learned visual representation with strong few-shot transfer capabilities

    - A single example per class on the ImageNet dataset (with 1000 classes), our best model achieves 69.52% accuracy
    - With 10 examples per class it attains 84.86%

    - We substantially reduce the memory footprint of the original ViT model
    - We train a model with 2b params and attain a new SOTA 90.45% on ImageNet


Core Results
    - We first present our main results on scaling trends
    - Compute is measured in TPUv3 core-days
    - We measure (i) few-shot transfer via training a linear classifier on frozen weights
    - (ii) transfer via fine-tuning the whole model on all data, both to multiple benchmark tasks
Scaling up Compute, Model and Data together
    - Doing all three improves representation quality
    - At the largest size, the models start to saturate and fall behind the power law frontier
    - Representation quality can be bottlenecked by model size
        - Small models are not able to benefit from either the largest dataset or compute resources
    - Large models benefit from additional data, even beyond 1B images
        - When scaling up the model size, the representation quality can be limited by smaller datasets
        - When increasing the dataset size, we observe a performance boost with big models, but now small ones
        - The largest models even obtain a performance improvement the training set size grows from 1b to 3b images
Double-saturating power law
    - Fig 2: Representation quality, measured as ImageNet finetune and linear 10-shot error rate, as a function of training compute
    - A saturating power-law approximates the pareto frontier fairly accurately
    - For over two orders of magnitude of compute, the relationship between compute and performance follows a power-law


Method Details
    - Improvements to the ViT model and training
    - An entire ViT-G/14 sitting on a single TPUv3 core
Decoupled weight decay for the "head"
    - Weight decay has a drastic effect on model adaptation in the low-data regime
    - We conduct a study of this phenomena at a mid-size scale
    - One can benefit from devoupling weight decay strength for the final linear layer "head" and for the remaining weights "body"
    - The best performance appears off-diagonal (decoupled weight decay for the head and body)
    - High weight decay in the head decreases performance on the pre-training (upstream) task, despite improving transfer performance
Saving memory by removing [class] token
Memory-efficient optimizers
    - Storage required for optimizers becomes a bottleneck
    - ViT-G 2B takes up 8gb, Adam stores two additional fp scalars per each parameter -> an additional 16gb needed
    - We explore two modifications
        1. Adam w/half-precision momentum:
            - Storing momentum in half-precision does not affect training dynamics
            - We can reduce optimizer overhead from 2-fold to 1.5-fold
        2. Adafactor optimizer:
            - Stores momentum using rank 1 factorization
            - Results in negligible memory overhead
            - Did not work out of the box, so we make the following modifications
                a. Reintroduce the first momentum in half-precision... (recommended setting does not use first momentum at all)
                b. Disable scaling of learning rate relative to weight norms
                c. Adafactor gradually increases the second momentum from 0.0 to 1.0 throughout the course of training. 
                    - It seems clipping the second momentum at 0.999 (Adam's default value) results in better convergence, so we adopt this
Learning-rate schedule
    - Explore learning-rate schedules that utilize a warmup and cooldown phase
    - Cooldown after approximately 200k, 400k, or 500k steps
    - Linear schedule best if one knows the training duration in advance
    - All three alternatives come reasonably close
Selecting model dimensions
    - Patch-size, number of encoder blocks (depth), dimensionality of patch emb and sa (width), number of attention heads, hidden-dim of MLP blocks
    - 