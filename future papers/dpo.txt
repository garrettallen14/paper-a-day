Direct Preference Optimization: Your Language Model is Secretly a Reward Model

Abstract

Large unsupervised LMs learn broad world knowledge and some reasoning skills, but achieving precise control of their behavior is difficult due to the unsupervised nature of their training.
Existing methods collect human labels (RLHF).
RLHF is complex and often unstable, first fitting a reward model, then fine-tuning the large unsupervised LM using RL to maximize this estimated reward w/o drifting too far from the original model.

We introduce a new parameterization of the reward model in RLHF which enables extraction of the corresponding optimal policy in closed form. This allows us to solve the standard RLHF problem with only a simple classification loss.

Direct Preference Optimization (DPO) is stable, performant, computationally lightweight.
No need to sample the LM during fine-tuning or performing significant hyperparameter tuning.

DPO can fine-tune LMs to align with human preferences as well as or better than existing methods.

Fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches/improves response quality in summarization and single-turn dialogue, all while substantially simpler to implement / train.


1 Introduction
LMs are trained on data generated by humans, so noisy.
We would like to bias our model toward the high-quality outputs present in its training data.

Similarly, we would want our LM to be aware of a common misconception believed by 50% of people, but we do not want the model to claim this misconception to be true in 50% of queries!

So, selecting the model's desired responses and behavior from its very wide knowledge and abilities is crucial in building AI systems.

Existing methods steer LMs to match human preference through RL.
We show that the RL-based objective used by existing methods can be optimized exactly with a simple binary cross-entropy objective, greatly simplifying the preference learning pipeline.

At a high-level, existing methods instill the desired behaviors into the LM through curated sets of human preferences.
This preference learning stage occurs after a general unsupervised pre-training stage.
The most successful class of methods is RLHF.
RLHF pipeline is considerably more complex than supervised learning.

In this paper, we show how to directly optimize an LM to adhere to human preferences without explicit reward modeling / RL.
We propose DPO, an algorithm that implicitly optimizes the same objective as existing RLHF algorithms, but is simple to implement and straight-forward to train.

Intuition: the DPO update increases the relative log probability of preferred to dispreferred responses. But, this update incorporates a dynamic, per-example importance weight that prevents the model degeneration that occurs with a naive probability ratio objective.

DPO relies on a theoretical preference model that measures how well a given reward function aligns with empirical preference data.
DPO uses a change of variables to define the preference loss as a function of the policy directly.
DPO can optimize a policy using a simple binary cross entropy objective, producing the optimal policy to an implicit reward function fit to the preference data.

