LaT-PFN: A Joint Embedding Predictive Architecture
for In-context Time-series Forecasting

16 May 2024

Abstract
    - Latent Time PFN, a foundational Time Series model w a strong embedding space enabling zero-shot forecasting
    - Perform ICL in latent space utilizing a novel integration of Prior Fitted Networks and JEPA
    - Create a prediction-optimized latent representation of the underlying stochastic process that generates time series and combines it with contextual learning
    - We improve on preceding works by utilizing related time series as a context and introducing an abstract time axis


Background
PFN: Prior-data Fitted Networks
    - Explicitly trains a model for in-context learning
    - NN approximates the posterior predictive distribution, thereby approximating Bayesian Inference
    - PFN requires a lot of data, so the PFN trains on synthetic data generated by a simulation, whose parameters are sampled from a user-defined prior dist


Methodology
Problem Statement
    - Seek to derive a formulation of time series forecasting for Bayesian Inference
Abstract Time Dimension
    - We map time to a fixed abstract interval, T0:TH
Architecture for Latent In-Context Forecasting
Embedder
    - Eight-layer dilated Mobilenet1D (conv)
Predictor
    - PFN predictor forecasts the next latent state, given the prompt and context
    - We apply learned average pooling AVGtheta over the seq dim of the embedded context D by cross-attn with a learned vector q
    - Use masking
Decoder
    - Three-layer ffwd network, trained in alignment with the JEPA methodology
System Identification Head
    - We add a regularization term to encourage consistency between history and forecast