Scaling Monosemanticity: 
Extracting Interpretable Features from Claude 3 Sonnet

21 May 2024

Abstract
    - Eight months ago, we showed Sparse Autoencoders can recover monosemantic features from a small one-layer transformer
    - Worry: this won't scale
    - We show we can extract high-quality features from Claude 3 Sonnet
    - Many features are multilingual and multimodal
    - Safety-relevant features: plausibly connected to a range of ways in modern AI systems may cause harm
    - Key Results:
        - Scaling laws can be used to guide the training of SAEs
        - Resulting features are highly abstract
        - Systematic relationship between frequency of concepts and the dictionary size needed to resolve features for them
        - Features can be used to steer LMs


Scaling Dictionary Learning to Claude 3 Sonnet
    - Linear representation hypothesis: neural networks represent meaningful concepts - features - as directions in their activation spaces
    - Superposition: NNs use the existence of almost orthogonal directions to represent more features than there are dimensions
    - We use dictionary learning


Sparse Autoencoders
    - Encoder maps the activity to a higher-dim layer via a learned linear transformation followed by a ReLU nonlinearity
        - The units of this high-dim layer as "features"
    - Decoder attempts to reconstruct the model activations via a linear transformation of the feature activations
        - Model trained to miniize a combination of
            1. Reconstruction error
            2. L1 regularization penalty on the feature activations, incentivizing sparsity
    - SAE provides us with approx decomp of the model's activations into a linear combination of "feature directions" (SAE decoder weights)
        - Coeff equal to the feature activations
        - Sparsity penalty ensures that for many given inputs to the model a very small fraction of features will have nonzero activations
    - For any given token in any context, the model activations are "explained" by a small set of active features

    - As a preprocessing step we apply a scalar normalization to the model activations so their average squared L2 norm is the residual stream dimension, D
    - We denote the normalized activations as x in RD and attempt to decompose this vector using F features as follows


Our SAE Experiments
    - We trained 3 SAEs:
        - 1M, 4M and 34M
        - Num training steps for the 34M feature run was selected using a scaling laws analysis to minimize the training loss given a fixed compute budget
    - Use an L1 coefficient of 5, perform a sweep over a narrow range of learning rates and chose the value that gave us the lowest loss
    - For all three SAEs, the avg number of features active (ie nonzero activations) on a given token was fewer than 300, and the SAE reconstruction explained at least 65% of the variance of the model activations
    - At end of training, we defined "dead" features as those which were not active over a sample of 10^7 tokens
    - The proportion of dead features was roughly 2% for the 1M SAE, 35% for the 4M SAE, 65% for the 34M SAE
        - We expect that improvements to the training procedure may reduce num of dead features in future experiments


Scaling Laws
    - Training SAEs on larger models is computationally expensive
    - Must understand
        1. Extent to which additional compute improves dictionary learning results
        2. How that compute should be allocated to obtain the highest-quality dictionary possible
    - Loss function is a useful proxy for assessing quality

    - Dictionaries with low loss values tend to produce interpretable features and improve other metrics of interest

    - With this proxy, we treat dictionary learning as a standard ML problem, to which we can apply "scaling laws" framework for hyperparameter optimization
    - Loss decreases approx according to a power law wrt compute

    - As compute budget increases, the optimal allocations of FLOPs to training steps / num features both scale approximately as power laws

    - In general, the optimal number of features appears to somewhat more quickly than the optimal num of training steps


Four Examples of Interpretable Features
    - Goal: demonstrate that interpretable features exist, leaving strong claims to a later section
    - Our interpretations are good descriptions of what the features represent and how they function in the network

    - Features:
        1. Golden Gate Bridge
        2. Brain sciences
        3. Monuments and popular tourist attractions
        4. Transit infrastructure

    - We show representative examples from the top 20 text inputs in our SAE dataset, as ranked by how strongly they activate that feature
    - A larger, randomly sampled set of activations can be found by clicking on the feature ID
    - The highlight colors indicate activation strength at each token
        - White -> no activation
        - Dark Orange -> strongest activation
    - Claims
        1. When feature is active, the relevant concept is reliably present in the context (specificity)
        2. Intervening on the feature's activation produces relevant downstream behavior (influence on behavior)


Specificity
    - Goal: rigorously measure the extent to which a concept is present in a text input

    - We focused on features that unambiguously corresponded to sets of tokens and computed the likelihood of that set of tokens relative to the rest of the vocab
    - Does not generalize to more abstract features

    - We use automation and construct the following rubric for scoring how a feature's description relates to the text on which it fires.
    - We ask Claude 3 Opus to rate feature activations at many tokens on that rubric.
        0 -- feature is completely irrelevant
        1 -- feature is related to the context, but not near the highlighted text / only vaguely related
        2 -- feature is only loosely related
        3 -- feature cleanly identifies the activating text

    - By scoring examples, we provide a measure of specificity for each feature

    - The features in this section are selected to have straightforward interpretations, to make automated interpretability analysis more reliable

    - Roughly 1000 activations of the feature drawn from the dataset are used to train the dictionary learning model
    - We plot the frequency of each rubric score as a function of the feature's activation level
        - Inputs that induce strong feature activations are all judged to be highly consistent with the proposed interpretation

    - Hard to quantify sensitivity - how reliably a feature activates for text that matches our proposed interpretation

    - We observe that the Golden Gate Bridge feature still fires strongly on the first sentence of the Wikipedia article for the GGB in various languages


Influence on Behavior
    - We experiment with feature steering
    - We "clamp" specific features of interest to artificially high / low values during the forward pass
    - We conduct these experiments with prompts in the "Human:"/"Assistant:" format that Sonnet is typically used with
    - We find feature steering is remarkably effective at modifying model outputs in specific, interpretable ways
    
    - Clamping GGB feature to 10x its maximum activation value induces thematically-related model behavior


Sophisticated Features
    - We have presented features in Claude 3 Sonnet that fire on relatively simple concepts
    - Code error feature
        - When clamped, hallucinates an error message!
        - When given negative activation, continues generation as if no bug was provided
            - Rewrites the code without the bug!!!
    - Features Representing Functions
        - Activates on functions that add numbers
            - Correctly handles function composition !!!


Features versus Neurons
    - Are the feature directions SAEs uncover more interpretable than, or even distinct from, the neurons of the model ?
    - We fit our SAEs on residual stream activity
    - Residual stream activity receives inputs from all preceding MLP layers
        - It could be the case that SAEs identify feature directions in the residual stream whose activity reflects the activity of individual neurons in the preceding layers

        - If this were the case, SAE would not be useful, as we could have identified the same features by simply inspecting MLP neurons
    - For a random subset of the features in our 1M SAE, we measure the Pearson correlation between its activations and those of every neuron in preceding layers
    - For the vast majority of features, there is no strongly correlated neuron
        - For 82% of our features, the most-correlated neuron has correlation of 0.3 or smaller

    - Even if dictionary learning features are not highly correlated with any individual neurons, it could still be the case that the neurons are interpretable
    - Upon manual inspection of a random smaple of 50 neurons and features, the neurons appear significantly less interpretable than the feautures, typically activating in multiple unrelated contexts


Feature Survey
    - Features are rich and diverse
    - We have millions of features. Scaling feature exploration is an important open problem
    - We have made some progress in characterizing the space of features, aided by automated interpretability
    - We focus on the local structure of features, which are often organized in geometrically-related clusters sharing a semantic relationship
    - We then turn to understanding more global properties of features, such as how comprehensively they cover a given topic or category
    - We then examine some categories of features we uncovered through manual inspection


Exploring Feature Neighborhoods
    - We walk through the local neighborhoods of several features of interest across the 1M, 4M and 34M SAEs, with closeness measured by the cosine similarity of the feature vectors
    - We find that this consistently surfaces features that share a relatede meaning or context -- the interactive feature UMAP has additional neighborhoods to explore

    - Distance in decoder space maps roughly onto relatedness in concept space
    
    - Feature splitting: features in smaller SAEs "split" into multiple features in a larger SAE, which are geometrically close and semantically related to the original feature, but represent more specific concepts


Feature Completeness
    - How complete is the space of concepts... Does the model have a feature corresponding to every major world city?
    - We used Claude to search for features which fired on members of particular families of concepts/terms
        1. We pass a prompt with the relevant concept (eg. "Feynman") to the model and see which features activate on the final token
        2. We take the top 5 features by activation magnitude and run them thru our automated interpretability pipeline
            - Ask Sonnet to provide explanations of what those features fire on
        3. We look at each of the top 5 explanations and a human rater judges whether the concept, or some subset of the concept, is specifically indicated by the model-generated explanation as the most important part of the feature

    - We find increasing convergence of concepts as we increase the number of features, though even in the 34M SAE, we see evidence that the set of features we uncovered is an incomplete description of the model internal representations
        - We confirmed Sonnet can list all of London boroughs, however we can only find features corresponding to about 60% of the boroughs in the 34M SAE
        - This suggests that the model contains many more features than we have found, which may be able to be extracted with even larger SAEs

    - What determines whether a feature corresponding to a concept is present in our SAEs?
        - Can look at freq of elements in a proxy of the SAE training data
        - We find that representation in our dictionaries is closely tied with the freq of the concept in the training data
        - Chemical elements often almost always have corresponding features in our dictionary
            - Those which are mentioned rarely or not at all do not
        - It is unclear to what extent feature learning is dependent on frequency in the model's training data rather than on the SAE's training data
        - Frequency in training data is measured by a search for <space>[Name]<space>

    - We quantified this relationship for four different categories of concepts:
        - Elements, cities, animals and foods
        - 100-200 concepts per category
        - Focused on concepts that could be unambiguously expressed by a single word and with a wide dist of frequencies in text data
        - We found a consistent tendency for larger SAEs to have features for concepts that are rarer in the training data, with "threshold" freq required for a feature to be present being similar across categories

    - The frequency in the training data at which the dictionary becomes more than 50% likely to include a concept is consistently slightly lower than the inverse of the number of alive features


Feature Categories
    - Through manual inspection, we identify a number of other interesting categories of features. Here we describe several in spirit of providing a flavor of what we see in our dictionaries rather than attempting to be complete or prescriptive
    - Person Features
    - Country Features
    - Basic Code Features
        - Highlighting different syntax elements or other low-level concepts in code
        - Beginnings of conditionals
        - Function arguments
        - Comments
        - Loop ranges
    - List Position Features
        - 1st entry, 2nd entry, ... all different features


Features as Computational Intermediates
    - Features let us examine the intermediate computation that the model uses to produce an output
    - We observe that in prompts where intermediate computation is required, we find active features corresponding to some of the expected intermediate results

    - A simple strat to identify causally important features: compute attributions, which are local linear approximations of the effect of turning a feature off at a specific location on the model's next-token prediction
    - We also perform feature ablations, clamping a feature's value to zero
    
    - We find the middle layer residual stream of the model contains a range of features causally implicated in the model's completion


Example: Emotional Inferences
    - John says, "I want to be alone right now." John feels
        - The model must parse the quote from John, identify his state of mind, and then translate that into a likely feeling.
    - If we sort features by either their attribution or their ablation effect on the completion "sad" as opposed to "happy" the top two features are:
        - A need or desire to be alone
        - Expressions of sadness
    - Both features contribute to the output... this implies that the model has partialy predicted a sentiment from John's statement but will do more downstream processing on the content of his statement (as represented by the first feature) as well

    - In comparison, the features with the highest avg activation on the context are less useful for understanding how the model actually predicts the next token in this case
        - The top feature is the same as given by attributions
        - 2nd and 3rd features are less abstract
            - One fires on "be" in "want to be" and variants
            - One fires on the word "alone"
Example: Multi-Step Inference
    - We investigate an incomplete prompt requiring a longer chain of inferences:
        - Fact: The capital of the state where Kobe Bryant played basketball is
    - To continue this text, the model must identify where Kobe played basketball, what state that place was in, and the capital of that state
    - We compute attributions and ablation effects for the completion "Sacramento" wrt the baseline "Albany"
    - Top 5 features:
        - Kobe feature
        - California feature
            - Activates most strongly on text after "California" is mentioned, rather than "California" itself
        - A "capital" feature
        - LA feature
        - Lakers feature

    - Depending on the choice of baseline token, we find attribution and ablation can surface less obviously completion-relevant features


Searching for Specific Features
    - SAEs contain too many features to inspect exhaustively
    - It is necessary to develop methods to search for features of particular interest, such as those relevant for safety, or those that provide special insight into the abstractions and computations used by the model

Single Prompts
    - Targeted prompts
        - We simply supplied a single prompt that relates to the concept of interest and inspect the features that activate most strongly for specific tokens in that prompt
        - Much more effective by automated interp labels, which made it easier to get a sense of what each feature represents at a glance

Prompt Combinations
    - Can select for features using sets of prompts
    - We filter for features active for all prompts in the set
    - Often include several "negative" prompts and filtered for features that were also not active in those prompts

    - Filtering via negative prompts was especially important when using images, as we found a set of content-nonspecific features which often activated strongly across many image prompts

Geometric Methods
    - We uncovered some interesting features by exploiting the geometry of the feature vectors of the SAE
    - Nearest neighbor features w/high cosine similarity

Attribution
    - We selected features based on estimates of their effect on model outputs
    - We sorted features by the attribution of the logit difference between two possible next-token completions to the feature activation

Safety-Relevant Features
    - We report discovery of unsafe features
    - Unsafe code, bias, sycophancy, deception and power seeking, dangerous or criminal information
    - Safety-Relevant Code Features
    - Bias Features
    - Sycophancy Features
    