
Update on how we train SAEs
Tom Conerly, Adly Templeton, Trenton Bricken, Jonathan Marcus, Tom Henighan
Weâ€™ve made improvements to how we train SAEs since Towards Monosemanticity with the goal of lowering the SAE loss. While the new setup is a significant improvement over what we published in Towards Monosemanticity we believe there are further improvements to be made. We havenâ€™t ablated every decision so itâ€™s likely some simplifications could be made. This work was explicitly focused on lowering loss and didnâ€™t grapple with loss not being the ultimate objective we care about. Hereâ€™s a summary of our current SAE training setup:

Let 
ğ‘›
n be the input and output dimension and 
ğ‘š
m be the autoencoder hidden layer dimension. Let 
ğ‘ 
s be the size of the dataset. Given encoder weights 
ğ‘Š
ğ‘’
âˆˆ
ğ‘…
ğ‘š
Ã—
ğ‘›
W 
e
â€‹
 âˆˆR 
mÃ—n
 , decoder weights 
ğ‘Š
ğ‘‘
âˆˆ
ğ‘…
ğ‘›
Ã—
ğ‘š
W 
d
â€‹
 âˆˆR 
nÃ—m
 , and biases 
ğ‘
ğ‘’
âˆˆ
ğ‘…
ğ‘š
,
ğ‘
ğ‘‘
âˆˆ
ğ‘…
ğ‘›
b 
e
â€‹
 âˆˆR 
m
 ,b 
d
â€‹
 âˆˆR 
n
 , the operations and loss function over a dataset 
ğ‘‹
âˆˆ
ğ‘…
ğ‘ 
,
ğ‘›
XâˆˆR 
s,n
  are:

ğ‘“
(
ğ‘¥
)
=
ReLU
(
ğ‘Š
ğ‘’
ğ‘¥
+
ğ‘
ğ‘’
)
ğ‘¥
^
=
ğ‘Š
ğ‘‘
ğ‘“
(
ğ‘¥
)
+
ğ‘
ğ‘‘
ğ¿
=
1
âˆ£
ğ‘‹
âˆ£
âˆ‘
ğ‘¥
âˆˆ
ğ‘‹
âˆ£
âˆ£
ğ‘¥
âˆ’
ğ‘¥
^
âˆ£
âˆ£
2
2
+
ğœ†
âˆ‘
ğ‘–
âˆ£
ğ‘“
ğ‘–
(
ğ‘¥
)
âˆ£
âˆ£
âˆ£
ğ‘Š
ğ‘‘
,
ğ‘–
âˆ£
âˆ£
2
f(x)
x
^
 
L
â€‹
  
=ReLU(W 
e
â€‹
 x+b 
e
â€‹
 )
=W 
d
â€‹
 f(x)+b 
d
â€‹
 
= 
âˆ£Xâˆ£
1
â€‹
  
xâˆˆX
âˆ‘
â€‹
 âˆ£âˆ£xâˆ’ 
x
^
 âˆ£âˆ£ 
2
2
â€‹
 +Î» 
i
âˆ‘
â€‹
 âˆ£f 
i
â€‹
 (x)âˆ£âˆ£âˆ£W 
d,i
â€‹
 âˆ£âˆ£ 
2
â€‹
 
â€‹
 

Note that the columns of 
ğ‘Š
ğ‘‘
W 
d
â€‹
  have an unconstrained L2 norm (in Towards Monosemanticity they were constrained to norm one) and the sparsity penalty (second term) has been changed to include the L2 norm of the columns of 
ğ‘Š
ğ‘‘
W 
d
â€‹
 . We believe this was the most important change we made from Towards Monosemanticity.

ğ‘
ğ‘’
b 
e
â€‹
  and 
ğ‘
ğ‘‘
b 
d
â€‹
  are initialized to all zeros. The elements of 
ğ‘Š
ğ‘‘
W 
d
â€‹
  are initialized such that the columns point in random directions and have fixed L2 norm of 0.05 to 1 (set in an unprincipled way based on 
ğ‘›
n and 
ğ‘š
m, 0.1 is likely reasonable in most cases). 
ğ‘Š
ğ‘’
W 
e
â€‹
  is initialized to 
ğ‘Š
ğ‘‘
ğ‘‡
W 
d
T
â€‹
 .

The rows of the dataset 
ğ‘‹
X are shuffled. The dataset is scaled by a single constant such that 
ğ¸
ğ‘¥
âˆˆ
ğ‘‹
[
âˆ£
âˆ£
ğ‘¥
âˆ£
âˆ£
2
]
=
ğ‘›
E 
xâˆˆX
â€‹
 [âˆ£âˆ£xâˆ£âˆ£ 
2
â€‹
 ]= 
n
â€‹
 . The goal of this change is for the same value of 
ğœ†
Î» to mean the same thing across datasets generated by different size transformers.

During training we use Adam optimizer beta1=0.9, beta2=0.999 and no weight decay. Our learning rate varies based on scaling laws, but 5e-5 is a reasonable default. The learning rate is decayed linearly to zero over the last 20% of training. We vary training steps based on scaling laws, but 200k is a reasonable default. We use batch size 2048 or 4096 which we believe to be under the critical batch size. The gradient norm is clipped to 1 (using clip_grad_norm). We vary 
ğœ†
Î» during training, it is initially 0 and linearly increases to its final value over the first 5% of training steps. A reasonable default for 
ğœ†
Î» is 5.

We do not use resampling or ghost grads because less than 1% of our features are dead at the end of training (dead means not activating for 10 million samples). We donâ€™t do any fine tuning after training.

Conceptually a featureâ€™s activation is now 
ğ‘“
ğ‘–
âˆ£
âˆ£
ğ‘Š
ğ‘‘
,
ğ‘–
âˆ£
âˆ£
2
f 
i
â€‹
 âˆ£âˆ£W 
d,i
â€‹
 âˆ£âˆ£ 
2
â€‹
  instead of 
ğ‘“
ğ‘–
f 
i
â€‹
 . To simplify our analysis code we construct a model which makes identical predictions but has an L2 norm of 1 on the columns of 
ğ‘Š
ğ‘‘
W 
d
â€‹
 . We do this by 
ğ‘Š
ğ‘’
â€²
=
ğ‘Š
ğ‘’
âˆ£
âˆ£
ğ‘Š
ğ‘‘
âˆ£
âˆ£
2
W 
e
â€²
â€‹
 =W 
e
â€‹
 âˆ£âˆ£W 
d
â€‹
 âˆ£âˆ£ 
2
â€‹
 , 
ğ‘
ğ‘’
â€²
=
ğ‘
ğ‘’
âˆ£
âˆ£
ğ‘Š
ğ‘‘
âˆ£
âˆ£
2
b 
e
â€²
â€‹
 =b 
e
â€‹
 âˆ£âˆ£W 
d
â€‹
 âˆ£âˆ£ 
2
â€‹
 , 
ğ‘Š
ğ‘‘
â€²
=
ğ‘Š
ğ‘‘
âˆ£
âˆ£
ğ‘Š
ğ‘‘
âˆ£
âˆ£
2
W 
d
â€²
â€‹
 = 
âˆ£âˆ£W 
d
â€‹
 âˆ£âˆ£ 
2
â€‹
 
W 
d
â€‹
 
â€‹
  and 
ğ‘
ğ‘‘
â€²
=
ğ‘
ğ‘‘
b 
d
â€²
â€‹
 =b 
d
â€‹
 .

Potential areas for improvement
Our initialization likely needs improvement. As we increase 
ğ‘š
m the reconstruction loss at initialization increases. This may cause problems for sufficiently large 
ğ‘š
m. Potentially with improved initialization we could remove gradient clipping.

We havenâ€™t seen improvements in loss from resampling or ghost grads, but itâ€™s possible resampling â€œlow valueâ€ features would improve loss.

Itâ€™s plausible some sort of post training (for example Addressing Feature Suppression in SAEs) would be helpful.

Improving shrinkage is an area for improvement.

There are likely other areas for improvement we donâ€™t know about.

Results
Given a fixed dataset 
ğ‘‹
X as we increase 
ğ‘š
m the loss consistently decreases. Weâ€™ve been able to increase 
ğ‘š
m to single digit millions without issues. This holds across a variety of transformer sizes and mlp activations or the residual stream. Our setup from Towards Monosemanticity would frequently have higher loss, many dead features, or many nearly identical features when run with large values of 
ğ‘š
m.

We make changes to our training setup by looking at loss across a variety of values of 
ğœ†
Î», 
ğ‘š
m, transformer sizes, and mlp or residual stream runs. Weâ€™re generally excited by a change that consistently decreases loss by at least 1%, or a change with roughly equal loss that simplifies our training setup. With our setup, comparing runs on (L0, MSE) or (L0, % of MLP loss recovered) requires care because L0 can be unstable. For example weâ€™ve had cases where training twice as long with half the number of features leads to a <1% change in MSE and L1 but a 30% decrease in L0.

Here are some results from small models. All runs have 131,072 features, 200k train steps, batch size 4096. Note that L1 of f depends on our specific normalization of activations.

Type of Run

Lambda

L0(f)

L1(f)

Normalized MSE

Frac Cross Entropy Loss Recovered

1L MLP

2

99.62386

17.22560

0.03054

0.98305

1L MLP

5

38.68729

11.59591

0.06609

0.96398

1L MLP

10

20.06870

7.12194

0.13120

0.91426

4L MLP (layer 2)

2

264.02930

95.03488

0.06824

0.96824

4L MLP (layer 2)

5

69.92758

56.92384

0.12546

0.92904

4L MLP (layer 2)

10

26.48456

39.42661

0.18485

0.88438

4L Residual Stream (layer 2)

2

81.58595

30.37323

0.09543

0.9572

4L Residual Stream (layer 2)

5

33.23121

19.12259

0.16295

0.90443

4L Residual Stream (layer 2)

10

8.71466

12.53889

0.25455

0.83883


NOTES PORTION:

Improvements on training SAEs with goal of lowering SAE loss
    - New setup is significant improvement over "Towards Monosemanticity"
    - Have not ablated every decision, so it's likely simplifications can be made

    - n input and output dim, m the AE hidden layer dim
    - s dataset size
    - Encoder weights We in Rmxn, Decoder weights Wd in Rnxm
    - Biases be in Rm, bd in Dn
    - Operations and loss function over a dataset X in Rx,n are:
        - ReLU
        - L2 norm + Reconstruction loss (?)

    - be and bd are initialized to all zeros
    - Elements of Wd are initialized s.t. the columns point in random directions and have fixed L2 norm of 0.05 to 1

    - 1L MLPs, 4L MLPs, residual streams