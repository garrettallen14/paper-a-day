A Multimodal Automated Interpretability Agent

22 Apr 2024

Abstract
    - Multimodal Automated Interpretability Agent (MAIA) a system that uses neural models to automate neural model understanding tasks like feature interpretation and failure mode discovery
    - Equip a pre-trained vision-language model with a set of tools that support iterative experimentation on subcomponents of other models to explain their behavior
    - Include tools commonly used by human interpretability researchers:
        - Synthesizing and editing inputs
        - Computing maximally activating exemplars from real-world datasets
        - Summarizing and describing experimental results
    - Interpretability experiments proposed by MAIA compose these tools to describe and explain system behavior
    - We evaluate applications of MAIA to computer vision models
    - We first characterize MAIA's ability to describe (neuron-level) features in learned representations of images.
    - Across several trained models and a novel dataset of synthetic vision neurons with paired ground-truth descriptions, MAIA produces descriptions comparable to those generated by expert human experimenters
    - MAIA can aid in two additional interpretability tasks: reducing sensitivity to spurious features, and automatically identifying inputs likely to be mis-classified


Introduction
    - Interpretability understanding is slow and expensive to obtain
    - Automated interp has begun to address some of these limitations by using learned models themselves to assist with model understanding tasks--for ex. by assigning natural language descriptions to learned representations, which may then be used to surface features of concern
    
    - How can we build tools that help users understand models, while combining the flexibility of human experimentation with the scalability of automated techniques?

    - Paper proposes MAIA, which combines a pretrained vision-LM with an API containing tools designed for conducting experiments on deep networks
    
    - MAIA is prompted with an explanation task and designs an interpretability experiment that composes experimental modules to answer the query
    - We additionally introduce a novel dataset of synthetic vision neurons built from an open-set concept detector with ground-truth selectivity specified via text guidance

    - MAIA descriptions of both synthetic neurons and neurons in the wild are more predictive of behavior than baseline description methods, and in many cases on par w/human labels

    - MAIA also automates model-level interp tasks where descriptions of learned reps. produce actionable insights about model behavior
    - MAIA's iterative experimental approach can be applied to downstream model auditing and editing tasks
        - Spurious feature removal
        - Bias identification in a trained classifier
        - Both applications demonstrate the adaptability of MAIA across experimental settings


Related work
    - Interpreting deep features
        - Approaches to describing neurons use exemplars of their behavior as explanation, either by visualizing features they select for, or automatically categorizing maximally-activating inputs from real-world datasets
    - Automated interp
        - Produced open-ended descriptions of learned features in the form of Natural Language text
        - Often as unreliable as causal descriptions of model behavior w/o further experimentation


MAIA Framework
    - An agent that autonomously conducts experiments on other systems to explain their behavior
    - We build MAIA from a pretrained multimodal model w/the ability to process images directly
        - GPT-4V backbone
Tools class
    - Dataset exemplar generation
        - It is possible to characterize the prototypical behavior of a neuron by recording its activation values over a large dataset of images
        - MAIA runs such an experiment on the validation set of ImageNet and constructs the set of 15 images that maximally activate the system it is interpreting
        - MAIA often chooses to begin experiments by calling this tool
    - Image generation and editing tools
        - MAIA is equipped with a prompt generating diffusion model function
    - Image description and summarization tools
        - To limit confirmation bias, we use a multi-agent framework in which MAIA can ask a new instance of GPT-4V with no knowledge of experimental history to describe highlighted image regions in individual images
    - Experiment log


Evaluation
Neurons in vision models
    - Use MAIA to produce natural language descriptions of a subset of neurons across three vision architectures trained under different objectives (CNN, DINO, ViT, CLIP, ResNet-50)
    - For each model we eval descriptions of 100 units randomly sampled from a range of layers that capture features at different levels of granularity
    - We collect human annotations of a random subset (25%) of neurons labeled by MAIA and MILAN, in an experimental setting where human experts write programs to perform interactive analyses of neurons using the MAIA API.
        - Human experts receive the MAIA user prompt, write programs that run experiments on the neurons, and return neuron descriptions in the same format
    
    - We evaluate the accuracy of neuron descriptions produced by MAIA, MILAN, and human experts by measuring how well they predict neuron behavior on unseen test images