Berkeley Function-Calling Leaderboard (BFCL) Summary
Full source: https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html


Summary:
    - The benchmark is useful for evaluating whether a model can take a single arbitrary user question, and generate appropriate function calls.
    - For each question, the model will only generate one response, returning a set of 1 or more function calls which seek to answer the provided user query.


BFCL uses 3 main Evaluation Categories:
    1. "Abstract Syntax Tree Evaluation (AST)" simply parses the text of the LLM's response to check if it aligns with the ground truth function call.
    2. "Evaluation by Executing APIs" actually executes the LLM's response, making a call to an API. Scoring is done by comparing the API's executed response with the expected ground truth answer.
    3. "Relevance Detection" measures how often a model will hallucinate a function call in the absence of relevant functions.


Greater detail:
    1. Abstract Syntax Tree Evaluation (AST)
        - 1150 questions.
        - Model is prompted, and generates a response.
        - Hard-coded function will parse through the response:
            - Find one of the following errors:
                - Function mismatch
                - Required parameters missing
                - Parameter hallucination
                - Type mismatch
                - Parameter values mismatch
            - Else, answer is a valid function call
        - Example Answers:
            - {"calculate_triangle_area": {"base": [10], "height": [5], "unit": ["units", "unit"]}}
            - {"predict_house_price": {"bedrooms": [3], "bathrooms": [2], "area": [1800], "location": ["San Francisco", "San Francisco, CA"]}}
    2. Evaluation by Executing APIs
        - 310 questions.
        - Model is prompted, and generates a response.
        - Local environment will execute the LLM's function call response.
        - We generate the following metrics by comparing the execution result with our expected ground truth value.
        - Metrics:
            - Non-REST API:
                - Exact match: The API execution result exactly matches the expected ground truth.
                - Real-time match: For live, dynamic APIs, the execution result must be within (20%) from the expected ground truth result.
                - Structural match: The execution result simply matches the expected ground truth data type.
            - REST API:
                - Effective execution: API call successfully executed
                - Response type accuracy: API execution result type matches the expected ground truth type
                - JSON Key Consistency: Check for consistency in JSON key sets between generated and expected responses
    3. Relevance Detection
        - 240 questions.
        - Design scenarios where none of the provided functions are relevant and supposed to be invoked. The expected ground truth is the model has no function call.

System Prompts:
    SYSTEM_PROMPT_FOR_CHAT_MODEL = """
    You are an expert in composing functions. You are given a question and a set of possible functions.
    Based on the question, you will need to make one or more function/tool calls to achieve the purpose.
    If none of the function can be used, point it out. If the given question lacks the parameters required by the function, also point it out. You should only return the function call in tools call sections.
    """

    USER_MESSAGE_FOR_CHAT_MODEL = "Questions:{user_prompt}\nHere is a list of functions in JSON format that you can invoke:\n{functions}. Should you decide to return the function call(s), NO other text MUST be included."

    - For tool-based models, there is no system prompt, simply the user message.
    - For non-tool-based models, the system prompt is included.