Home

Resources
    - Can download SWE-bench task instances from HF or directly as a JSON file.
About
    - Test systems' ability to solve GitHub issues automatically
    - Dataset collects 2294 Issue-Pull request pairs from 12 popular Python repos
    - Evaluation is performed by unit test verification using post-PR behavior as the reference solution
    - SWE-bench Lite is a subset of SWE-bench that makes eval less costly and more accessible
    - SWE-bench Verified is a human annotator filtered subset that has been deemed to have a ceiling of 100% resolution rate

    - % Resolved => percentage of SWE-bench instances that were resolved by the model
    - ! Checked => the SWE-bench team received access to the system and were able to reproduce the patch generations
    - Open => open-source code submissions

    - All submissions are Pass@1, do not use hints_text and are unassisted


Submit
    - To submit your model to SWE-bench Leaderboard:
        1. Fork the SWE-bench/experiments repository
        2. Clone the repo
        3. Under the split that you evaluate on (eval/lite or eval/test), create a new folder with the submission date and model name (20240415_swe_agent)
        4. Within the folder, please include the following lines:
            - all_preds.jsonl: model predictions
            - logs/: SWE-bench eval artifacts dump
                - Eval artifacts => 300/2294 (Lite / Test) folders
                - Each folder contains:
                    - eval.sh: eval script
                    - patch.diff: model's generated prediction
                    - report.json: summary of evaluation outcomes for this instance
                    - run_instance.log: a log of SWE-bench evaluation steps
                    - test_output.txt: an output of running eval.sh on patch.diff
                - NOTE: You shouldn't have to create any of these files. They should automatically be generated by SWE-bench evaluation.
            - metadata.yaml: Metadata for how result is shown on website:
                - name: name of leaderboard entry
                - oss: true if open-source
                - site: url/link to more info
                - verified: false
            - trajs/: reasoning trace reflecting how your system solved the problem
                - One reasoning trace per task instance. Should show all of the steps your system took while solving the task. If your system outputs thoughts or comments during operation, they should be included as well.
                - The reasoning trace can be represented with any text based file format (md, json, yaml)
                - Ensure the task instance ID is in the name of the corresponding reasoning trace file
                - For an example, see SWE-agent + GPT 4 Turbo
            - README.md: Include anything you'd like to share about your model here!
        5. Run python -m analysis.get_results eval/split/date_model
        6. Create a pull request to the SWE-bench/experiments repository with the new folder.
    - You can refer to this tutorial for a quick overview of how to evaluate your model on SWE-bench
Submission Guidelines
    - We consider an eligible submission to the SWE-bench leaderboard to satisfy:
        1. The use of hints_text is not allowed
        2. The result is pass@1. One execution log per task instance for all 2294 task instances
        3. The result should not be in the "Oracle" retrieval setting. The agent cannot be told the correct files to edit, where "correct" refers to the files modified by the reference solution patch.
Verify Your Results
    1. Create an issue
    2. In the issue, provide instructions on how to run your model on SWE-bench
    3. We will run your model on random subset of SWE-bench and verify the results
Reasoning Traces
    - We have updated the SWE-bench leaderboard submission criteria to require the inclusion of reasoning traces
        - Provides the community with more insight to how cutting edge methods work without requiring a code release
    - Text-based files that describe your steps to solve a task instance
    - No specific guidelines, but should be:
        - Human readable
        - Reflects the intermediate steps your system took that led to the final solution
        - Generated with the inference process, not post-hoc
    - We do not require reasoning traces to be...
        - In a specified file format (json, yaml, md)


Containerized Evaluation Harness
    - We hypothesized that conda environments would be enough to enforce reproducible evaluation. In hindsight, it is underspecified.
        - SWE-bench evaluation remained sensitive to discrepancies originating from different platforms and user-specific configurations, leading to inconsistent results.
    - Our new harness provisions ***per-sample Docker images with Python virtual environments*** that have been rigorously tested.
        - In the new Docker harness, 99.78% (2289/2294) of SWE-bench tasks and 100% (300/300) of SWE-bench Lite tasks consistently resolve correctly with the ground truth solution.
Running Evaluation
    - The main entrypoint for the evaluation harness is the swebench.harness.run_evaluation module.
        - python -m swebench.harness.run_evaluation -h
    - Runs docker containers for each evaluation instance in parallel. In the process of running the evaluation, the harness will:
        1. Build a base image that install basic dependencies for all instances
        2. Build "environment" images that initialize the python env for various configs that are common to multiple instances (60 => 100 GB of images)
        3. Build "instance" images that install the specific dependencies and source code for each instance
    - The harness will then run the evaluation script in each instance container, and collect the results. After the evaluation is complete, the harness will clean up the containers and images depending on the --cache_level argument
Choosing the right cache_level
    - The harness builds images for each instance
    - We provide a cache_level argument to control how the harness caches images between runs
    - By default, the harness cache_level is set to env
        - The harness will store the base and environment images, but not the instance images
        - In this setting, the base and environment images will be reused across runs, but take up about 100GB of disk space.
    - Fastest possible evaluation times, we recommend setting cache_level to instance
    - However, all base, environment, and instance images will be stored, taking up about 2000GB of disk space
    - To minimize disk space usage, we set cache_level to base or none.
Choosing the right max_workers
    - Should be chosen based on the resources available on your machine
Future Steps
    - Soon we want to make the harness even more user-friendly by providing pre-built docker images that include verified starting points for each instance.
    - We're also hoping to better enable evaluation via orchestration tools like Kubernetes, which would allow users to run evaluations on larger clusters of machines
    - We're providing experimental support for ...


https://github.com/CognitionAI/devin-swebench-results/tree/main