G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment

23 May 2023

Abstract
    - LLM-based evaluators have lower human correspondence than medium-sized neural evaluators
    - We present G-Eval, a framework of using LLMs w/CoT and a form-filling paradigm to assess the quality of NLG outputs
    - G-Eval with GPT-4 backbone achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods


Introduction
    - Feed task introduction and evaluation criteria as a prompt
    - Ask LLM to generate a CoT of detailed evaluation steps
    - We use the prompt along with the generated CoT to evaluate NLG outputs

    - Evaluator output is formatted as a form
    - The probabilities of the output rating tokens can be used to refine the final metric


Method
    - Prompt-based evaluator with 3 main components:
        1. Prompt that contains the definition of eval task and desired eval criteria
        2. CoT that is a set of intermediate instructions generated by the LLM describing the detailed eval steps
        3. Scoring function that calls LLM and calculates the score based on the probabilities of the return tokens
    - Prompt for NLG Evaluation
        - Ex: "You will be given one summary written
               for a news article. Your task is to rate
               the summary on one metric.

               Please make sure you read and under-
               stand these instructions carefully. Please
               keep this document open while reviewing,
               and refer to it as needed.
               
               Evaluation Criteria:
               Coherence (1-5) - the collective quality
               of all sentences. We align this dimen-
               sion with the DUC quality question of
               structure and coherence whereby ”the
               summary should be well-structured and
               well-organized. The summary should not
               just be a heap of related information, but
               should build from sentence to sentence
               to a coherent body of information about
               a topic.”
    - Auto CoT for NLG Evaluation
        - We prompt the LLM to generate the following CoT automatically
    - Scoring function
        - The scoring function calls the LLM with the designed prompt, auto CoT, the input context and the target text that needs to be evaluated.
        - G-Eval directly performs the evaluation task with a form-filling paradigm
        - For example, for evaluating coherence in text summarization, we concatenate the prompt, the CoT, the news article, and the summary, and then call the LLM to output a score from 1 to 5 for each eval aspect, based on the defined criteria.
        - However, we notice this direct scoring has two issues:
            1. One digit often dominates the distribution of scores
            2. LLMs only output integer scores, even when we request decimals

        - We propose using the probabilities of output tokens from LLMs to normalize the scores and take their weighted summation as the final results
        - Given a set of scores (like 1 to 5) predefined in the prompt, the probability of each score is calculated by the LLM. The final score is:
            score = sum........