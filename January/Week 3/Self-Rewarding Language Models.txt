Self-Rewarding Language Models
18 Jan 2024

Abstract
- For superhuman agents, future models require super-human feedback to provide an adequate training signal.
- We study self-rewarding LMs, where the LM itself is used via LLM-as-a-Judge prompting to provide its own rewards.
- During iterative DPO training, not only does instruction following improve, but also the ability to provide high-quality rewards to itself.
- Fine-tuning Llama 2 70B on three iterations yields a model outperforming many systems.

- This work opens the door to the possibility of models that can continually improve in both providing high-quality rewards and following instructions.


Introduction
- Aligning LLMs w/HF can vastly improve instruction following performance of pre-trained models.
- Current approach is bottlenecked by size / quality of the Human Preference data
- We propose to train a self-improving reward model that, rather than being frozen, is continually updating during LLM alignment to avoid this bottleneck.

- The key to such an approach is to develop an agent that possesses all abilities desired during training, rather than separating them out into distinct models like reward and LM.
- Incorporating the reward model allows task transfer between reward modeling task and the instruction following tasks.

- We thus introduce Self-Rewarding LMs, agents that both:
(i) act as instruction following models, generating responses for given prompts;
(ii) can generate and evaluate new instruction following examples to add to their own training set.
- We train these models using an Iterative DPO framework similar to Xu et al. 2023.
- Start from seed model, in each iteration there is a process of Self-Instruction creation whereby candidate responses are generated by the model for newly created prompts.
- These candidate responses are then assigned rewards by that same model (LLM-as-a-Judge prompting)
- A preference dataset is built from the generated data, and the next iteration of the model is trained via DPO.

- Start with Llama 2 70B seed model fine-tuned on Open Assistant
- Perform the training scheme.
- We find that the instruction following performance improves from Self-Rewarding LLM alignment, but the reward modeling improves as well!

- This means that the model during iterative training is able to provide a higher quality perference dataset to itself than in the previous iteration.


Self-Rewarding LMs
- Our approach assumes access to a base pretrained LM, and a small amount of human-annotated seed data.
- We then build a model that aims to possess two skills:
1. Instruction following: given a prompt that describes a user request, the ability to generate a high quality, helpful (and harmless) response.
2. Self-Instruction creation: the ability to generate and evaluate new instruction-following examples to add to its own training set.

- These skills are used so the model can perform self-alignment (iterative training w/ AIF (AI Feedback))

- Self-instruction creation:
1. Generating candidate responses
2. The model judges their quality
- This self-created AIF preference data is used as a training set.

- Self-alignment procedure is iterative.

Initialization
- We are given a seed set of instruction, prompt, response examples to use to train in a SFT manner
- We also assume we are provided w/ a seed set of (eval instruction prompt, eval result response) which can also be used for training
- We used both these seed sets together during training

Self-Instruction Creation
- Using the newly trained model, we can make it self-modify its own training set.
- Specifically, we can generate additional training data for the next iteration.
- Following steps:
1. Generate a new prompt: generate a new prompt xi using few-shot prompting, sampling prompts from the original seed IFT data
2. Generate candidate responses: We then generate N diverse candidate responses {y1i,...,yNi} for the given prompt xi from our model using sampling.
3. Evaluate candidate responses: Finally, we can use the LLM-as-a-Judge to evaluate its own candidate responsese with scores rni in [0,5]

Instruction Following Training
- AI Feedback Training: After performing the self-instruction creation procedure, we can augment the seed data with additional examples for training
- We refer to this as AI Feedback Training (AIFT) data.
- We try two variants of such feedback:
1. Preference pairs: we construct training data of the form (inst prompt xi, winning resp ywi, losing resp yli)
- To form the winning and losing pair, we take the highest and lowest scoring responses from the N evaluated candidate responses
- We discard the pair if their scores are the same
- These pairs can be used for training with a preference tuning algorithm... we use DPO.
2. Positive examples only:
- In this variant, we add additional examples of (inst, prompt, response) curated by the model to the seed set for SFT...

Overall Self-Alignment Algorithm
Iterative Training:
- Procedure trains a series of models M1,...,MT where each successive model t uses augmented training data created by the t-1 model.
Model Sequence:
- M0 = base pretrained
- M1 = initialized w/M0, fine-tuned on IFT+EFT seed data using SFT.
- M2 = initialized w/M1, trained with AIFT(M1) data using DPO
- M3 = initialized w/M2, trained with AIFT(M2) data using DPO

