Direct Preference Optimization: Your Language Model is Secretly a Reward Model

Abstract

- Large unsupervised LMs learn broad world knowledge and some reasoning skills, but achieving precise control of their behavior is difficult due to the unsupervised nature of their training.
- Existing methods collect human labels (RLHF).
- RLHF is complex and often unstable, first fitting a reward model, then fine-tuning the large unsupervised LM using RL to maximize this estimated reward w/o drifting too far from the original model.
- We introduce a new parameterization of the reward model in RLHF which enables extraction of the corresponding optimal policy in closed form. This allows us to solve the standard RLHF problem with only a simple classification loss.
- Direct Preference Optimization (DPO) is stable, performant, computationally lightweight.
- No need to sample the LM during fine-tuning or performing significant hyperparameter tuning.
- DPO can fine-tune LMs to align with human preferences as well as or better than existing methods.
- Fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches/improves response quality in summarization and single-turn dialogue, all while substantially simpler to implement / train.


Introduction
- LMs are trained on data generated by humans, so noisy.
- We would like to bias our model toward the high-quality outputs present in its training data.
- Similarly, we would want our LM to be aware of a common misconception believed by 50% of people, but we do not want the model to claim this misconception to be true in 50% of queries!
- So, selecting the model's desired responses and behavior from its very wide knowledge and abilities is crucial in building AI systems.
- Existing methods steer LMs to match human preference through RL.
- We show that the RL-based objective used by existing methods can be optimized exactly with a simple binary cross-entropy objective, greatly simplifying the preference learning pipeline.

- At a high-level, existing methods instill the desired behaviors into the LM through curated sets of human preferences.
- This preference learning stage occurs after a general unsupervised pre-training stage.
- The most successful class of methods is RLHF.
- RLHF pipeline is considerably more complex than supervised learning.
- In this paper, we show how to directly optimize an LM to adhere to human preferences without explicit reward modeling / RL.
- We propose DPO, an algorithm that implicitly optimizes the same objective as existing RLHF algorithms, but is simple to implement and straight-forward to train.
- DPO update increases the relative log probability of preferred to dispreferred responses. But, this update incorporates a dynamic, per-example importance weight that prevents the model degeneration that occurs with a naive probability ratio objective.
- DPO relies on a theoretical preference model that measures how well a given reward function aligns with empirical preference data.
- DPO uses a change of variables to define the preference loss as a function of the policy directly.
- DPO can optimize a policy using a simple binary cross entropy objective, producing the optimal policy to an implicit reward function fit to the preference data.

- DPO is a simple RL-free algorithm for training language models from preferences. Our experiments show that DPO is at least as effective as existing methods.


Preliminaries
- RLHF Pipeline includes three phases:
1. Supervised fine-tuning (SFT): 
- RLHF begins by fine-tuning a pre-trained LM w/supervised learning on high-quality data to obtain a model piSFT
2. Reward Modeling Phase:
- The SFT model is prompted with prompts x to produce pairs of answers (y1,y2) ~ piSFT (y | x)
- These answers are given to human labelers who express preference for one answer.
- The preferences are assumed to produce some latent reward model.
3. RL Fine-Tuning Phase: 
- During RL phase, we use the learned reward function to provide feedback to the LM.
- Due to the discrete nature of language generation, this objective is non-differentiable and typically optimized with RL.


Direct Preference Optimization
- Goal: derive a simple approach for policy optimization using preferences directly.
- Prior RLHF: learn reward -> optimize via RL
- New approach: leverages a particular choice of reward model that enables extraction of the optimal policy in closed form, without an RL training loop.
- We transform a loss function over reward functions to a loss function over policies.

Deriving the DPO objective
- Start with same RL objective as prior work under a general reward function r.
- The optimal solution to the reward maximization objective takes the form: ...
- This is expensive, however we can rearrange it to express the reward function as ...

What does the DPO update do?
- The gradient of the loss function L DPO increases the likelihood of the preferred completions yw, decreases the likelihood of dispreferred completions yl.

DPO outline
1. Sample completions y1,y2 ~ pi ref(.|x) for every prompt x, label with human preferences to construct the offline dataset of preferences
2. Optimize the language model to minimize L DPO


Theoretical Analysis of DPO

Your Language Model is Secretly a Reward Model
- DPO is able to bypass both fitting an explicit reward and performing RL to learn the policy using a single max likelihood obj.
