A Theory of Consciousness from a Theoretical Computer Science Perspective:  
Insights from the Conscious Turing Machine  
Lenore Blum and Manuel Blum

Abstract
- The quest to understand consciousness, examined from the perspective of Theoretical Computer Science (TCS)
- TCS, a branch of mathematics concerned with understanding the underlying principles of computation and complexity

- Not looking for a complex model of the brain nor of cognition
- Seeking a simple substrate independent computational model of (the admittedly complex concept of) consciousness

- We define the Conscious Turing Machine (CTM), also called Conscious AI
- Define consciousness and related notions in the CTM.
- Only mathematical definitions, but we suggest why the CTM has feelings of consciousness.
- The TCS perspective provides a simple framework to employ tools form computational complexity theory and ML to help us understand consciousness + related concepts

- Previously we explored explanations for feelings of pain / pleasure in the CTM. Here we consider additional phenomena generally associated with consciousness
- Start with three examples related to vision (blindsight, inattentional blindness, and change blindness)
- We follow then with a discussion of dreams, altered states, and free will.
- We give explanations derived from the model and draw confirmation from consistencies at a high level -- well above the level of neurons -- with the psychological and neuroscience literature.


Introduction: Why a Theoretical Computer Science Perspective?
- Humanity getting closer to understanding how the brain achieves consciousness (?)
- Bernard Baars -- Global Workspace Theory (GWT) of the brain.

- We study consciousness from the perspective of Theoretical Computer Science
- A branch of math concerned with understanding the underlying principles of computation.
- These principles largely include the complexity of computation
- This deals with the consequences and unexpected usefulness of taking resource limitations into account.

- We claim that TCS perspective and unique insights add to the understanding of consciousness and related concepts such as qualia and free will.
- We give a simple abstract substrate-independent computational model of consciousness that we call the Conscious Turing Machine (CTM)
- The CTM is inspired by Turing Machine, GWT, and Global Neuronal Workspace Theory (GNWT)

- Not looking to model the brain or suggest neural correlates of consciousness
- Simply looking to understand consciousness and how a machine might experience feelings.
- Intent is to come to terms with the hard problem (Chalmers 1995)

- Our view: consciousness is a property of all properly organized computing systems (!), flesh and blood or metal and silicon.

What is Theoretical Computer Science?
- Turing "On computable numbers" presents a formal mathematical definintion of a Turing Machine.
- Turing Machine -> P=?NP research -> TCS which defined computational complexity of problems

- We illustrate the concept of a computer-generated random sequence, called a pseudo-random sequence.
- Pseudo-random sequence: a polynomial time program for generating sequences which cannot be distinguished from truly random sequences by any feasible computer program.
- So, in the polyn time world we live, pseudo-random sequences are "truly" random.
- This understanding was impossible w/o TCS

- An application of the above ideas: replace the use of random sequences in the CTM by sequences produced by pseudo-random generators supplied with (short) random seeds.
- If the probabilistic CTM has "free will", as will be argued, then so does this deterministic CTM.
- The free will of a deterministic CTM is counter to some and perhaps much of the thinking on determinism.

Now for consciousness:
- The TCS perspective is employed in the definition of the CTM, a simple machine that formalizes a modified version of the GWT of consciousness.
- Baars describes consciousness through a theater analogy as the activity of actors in a play performing on the stage of Working Memory. 
- Their performance under observation by a huge audience of unconscious processors sitting in the dark
- In the CTM, the stage is represented by a STM that at any moment in time contains CTM's conscious content.
- The audience members are an enormous collection of powerful processors -- each with its own expertise -- that make up CTM's LTM.
- These LTM processors make predictions and get feedback from CTM's world.
- Based on this feedback, learning algorithms internal to each processor improve that processor's behavior.

- LTM processors, each with their own specialty, compete to get their questions, answers, and information in the form of chunks on the stage for immediate broadcast to the audience.
- Conscious awareness (elsewhere called attention) is defined formally in the CTM as the reception by the LTM processors of the broadcast of CTM's conscious content.
- These processors become connected by links over time, turning conscious communication (through STM) into unconscious communication (thru links) between LTM processors.
- Ignition: communication via links about a broadcasted chunk reinforces its conscious awareness

Complexity considerations enter into fixing the detailed definition of CTM:
1. Chunk: the information that each LTM processor puts into the competition for consciousness at each and every tick of the clock.
2. Probabilistic competition algorithm: selects which one of the many competing chunks reaches consciousness
3. ML algorithms in each processor that use feedback from global broadcasts, other processors, and outside world to update the CTM's competitiveness & reliability
4. Memory in each LTM processor is random access rather than linear access (FILO) of Turing machine tapes, because random access is needed for things like fast binary search.

- Complexity considerations, particularly ones of consequences of limited resources, play a crucial role in our high level explanations for consciousness-related phenomena such as change blindness & feeling of free will.

- What gives CTM its "feeling of consciousness" is the Global Workspace architecture, predictive dynamics, rich multi-modal inner language, and certain special LTM processors including an Inner generalized speech processor and Model-of-the-World processor

- Not looking for model of brain, but model of consciousness
- CTM again too simple for that.


1. The CTM Model
1.1 Basic Structure + Definition of "Consciousness in the CTM"
- CTM is a device defined by a 7-tuple: < STM, LTM, Up-Tree, Down-Tree, Links, Input, Output >
- CTM has a clock measuring time in discrete clock ticks (10 ticks per second, the alpha rhythm of the brain)
- CTM has a finite lifetime, born at t=0, dies at T=0.

1.1.1 STM & LTM processors
- In the CTM, the stage (conscious arena) is represented by a Short Term Memory. This is a small memory capable of holding a single chunk.
- The audience (in the unconscious arena) is represented by a massive collection of N > 10^7 powerful parallel random-access processors, each with their own random-access memory, each large enough to hold a small multiple of T chunks.
- This makes up the LTM. Each LTM processor runs a number of algorithms, one of which is the processor's personal Sleeping Experts algorithm.
- All processors are in the LTM, none in the STM.
- When we say processor, we always mean LTM processor.

- Certain special LTM processors are particularly responsible for CTM's "feelings of consciousness". These include a Model-of-the-World processor, among others...

1.1.2 The Up-Tree & Down-Tree
- Up-Tree: Up-directed binary tree of height h with N leaves, one leaf per LTM processor, and a single root in STM.
- Every directed path from leaf up to the root is of length h.
- Info in the Up-Tree travels from the leaves below to the root above.
- Down-Tree: simple down-directed tree of height 1, with a single root in STM and N edges directed from that root to the N leaves, one edge per processor, which carry information from the root to all N leaves.

- LTM processors, each with their own specialty, compete via the Up-Tree competition to get their Qs, As, and info in the form of chunks into STM.
- The competition takes h clock ticks.
- At each time t, all LTM processors submit information to the competition for STM.
- One of those processors wins access to STM at time t+h, and all processors recieve the winning broadcast from STM at time t+h+1 (?)

1.1.3 Conscious Content, Conscious Awareness & Streams of Consciousness
- The chunk that wins the Up-Tree competition (to get into the STM) that began at time t-h is called the conscious content of CTM at time t.
- We say that CTM becomes consciously aware of that conscious content, which appeared at time t in STM, when this conscious content is recieved by all LTM processors at the time t+1.
- Conscious awareness: the reception by all the LTM processors of STM's broadcast, rather than as the appearance in STM of the winning chunk
- This emphasizes the feeling of consciousness arises after all processors receive the broadcast and already act on it

- Our definition of conscious awareness aligns roughly with what cognitive neuroscientists call "attention"
- Ignition ~ Reverberation of chunks via processor links immediately following reception
- Reason to keep # of chunks in STM small: ensure all processors focus on the same information in the broadcast from the STM. (also keep model simple)

- CTM is constantly bubbling with the activity of chunks competing for STM, its winners being constantly broadcast from STM to LTM.
- Time-ordered chunks broadcast from STM to LTM form a stream of consciousness. This stream is part of the subjective "feeling of consciousness"

1.1.4 Links, Unconscious Communication & Global Ignition
- All communication between any two processors occurs initially (at birth and until links are formed) via STM.
- Processor A can submit a query to the Up-Tree competition. If the query wins the competition, it is broadcast to all processors.
- Processor B may then submit an answer via the competition, which if it wins gets broadcast, and so on.
- If A acknowledges that B's answer is useful (this occurs sufficiently often), then a bi-directional link forms (or existing one is strengthened between A and B)

- In addition to processors putting chunks into the Up-Tree competition, processors can send chunks through links.
- When chunks are sent though links taht formerly went through STM, conscious communication (thru STM) between A and B turns into unconscious communication between A and B.
- Links are channels for transmitting information between processors. Those chunks can reinforce / sustain conscious awareness. (!!!!!)
- Global ignition in the Global Neuronal Workspace Theory (GNWT)

1.1.5 Input / Output Maps, Sensors & Actuators
- CTM environment (Env) is a subset of Rm(t) (t is time)
- Input maps take environmental information acquired by CTM's sensors and send it to designated processors to convert into chunks.
- Output maps take chunks from designated processors, convert to command instructions, send to actuators to act on the environment.

1.1.6 Summary of Connections
Env -> LTM: directed edges from the environment via sensors to processors of the sensory data
LTM -> STM: via the Up-Tree edges
STM -> LTM: via the Down-Tree edges
LTM -> LTM: bi-directional edges (links) between processors
LTM -> Env: directed edges from specific processors (like those that generate instructions for finger movement) to the environment via actuators (like the fingers that receive instructions from these processors) that act on the environment. 

1.2 Brainish (CTM Multi-Model Inner Language), Chunks & Gits
- Brainish is the language used to communicate between processors (competition or thru links)
- Express inner speech, vision, sensations, imaginings, dreams.
- Includes coded representations of inputs / outputs
- All expressed w succinct multi-modal Brainish words and phrases called gists.
- Gists:
- can hold the essence of a scene or (high level expandable) idea of a proof.
- Can be an answer to a query
- An insight of some sort
- A dream image
- Description of pain...
- etc...
- Brainish can express and manipulate:
- images, sounds, tactile sensations, and thoughts (including unsymbolized thoughts)
- Does this BETTER than spoken outer languages
- Claim that an expressive inner language is an important component of the feeling of consciousness

- Information is carried on all edges between LTM processors, between STM and LTM, from input to LTM, and from LTM to output, by chunks.
- A chunk is a 6-tuple: < address, t, gist, weight, intensity, mood > where
1. Address is the address of the LTM processor that produced the chunk
2. t is the time the chunk was produced (put into competition)
3. Gist is the information "concisely expressed" in Brainish.
4. Weight is a valanced real number that the processor gives the gist
5. Intensity starts off as |weight|
6. Mood starts off as weight.

- Note: the size of a chunk (and hence the size of its components, including its gist) will necessarily be bounded by computational complexity considerations.

1.3 (Probabilistic) Up-Tree Competition, Coin-Flip Neuron & Competition Functions
- Up-Tree competition: determines which LTM processor gets its chunk into STM.
- At each clock tick t = 0,1,...,T the t competition starts with each processor p putting its chunk into its leaf of the Up-Tree.
- After a chunk is submitted, address,t,gist,weight remain unchanged, but intensity and mood are updated to incorporate ever more global information
- Deciding whether or not a chunk moves up one level or drops out is made by a fast tiny parallel circuit
- The probabilistic Up-Tree is the proper one to consider
- At each clock tick, the circuit in a non-leaf node runs a local competition that probabilistically selects one of v's two (sibling) children based on a comparison of the chunks they contain
- Moves (a variant) of that chunk into v. This chunk is the winner of the local competition at v.

- The local competition employs the Up-Tree's competition function f, mapping chunks to non-neg real numbers.

- Current mood of CTM at time t is defined to be the mood of the chunk that is broadcast from STM at time t...

- Definition: competition f is Additive if f(chunk(v)) = f(chunkL(v)) + f(chunkR(v))
- Theorem: If the competition fucntion is additive, then every chunk submitted to the Up-Tree competition gets a fraction of time in STM 
- As a consequence, for additive f, the permutation chosen has no effect on the sequence of broadcasts from STM

1.4 Complexity of Computation & Time Delay for Conscious Awareness
...

1.5 Memories & the High Level Story
- Assume each processor p stores in its internal memory at time t:
1. the chunk(p,t,0) submitted to the competition
2. the chunk recieved by broadcast from the STM
3. a select subset of chunks it received from links or from Input maps.
- These stores are a substantial portion of CTM's memories...

1.6 Predictive Dynamics = Prediction + Feedback + Learning (Sleeping Experts Algorithm)
- Processors require feedback to assert correctness, detect errors, and learn how to boost correctness!
1. Predictions in CTM are made by LTM processors both within their internal algorithms and implicitly when they submit chunks elsewhere (to STM, other processors thru links, or to actuators to effect the environment)
2. Feedback comes from chunks that are received in broadcasts from STM, thru links, and from sensors of the environment via Input maps, indicating correctness or detecting errors in predictions.
3. All learning / error correcting takes place within processors

- There is a continuous cycling of prediction, feedback, and learning within CTM.
- The CTM needs to be alert to anything unusual, surprises of any kind, in order to deal with things if necessary and understanding the world in any case.
- Prediction errors (eg "surprises") are minimized by this cycling.

- Processors especially need to know if they were too timid or bold in setting their |weights|, so they can correct their weight-assigning algorithms
- Sleeping Experts Algorithms (SEA): a class of learning algorithms employed by LTM processors to do just that.

- SEA will embolden its processor -- pushing it to raise the intensity it gives its chunks - if
1. its chunk did not get into the STM, and
2. its information is more valuable (in the SEA's opinion) than what got into the STM.

- SEA will hush its processor -- lowering the intensity of chunks - if
1. its chunk got into STM, and
2. the info in that chunk is found to be less valuable than that of another chunk that failed to get into the STM.

- SEAs play a role in whether or not processors get their chunks into the STM. 
- They also play a role in whether or not processors "pay attention" to gists in chunks that are sent to them via links.
- The |weight| of a chunk is an indication of how important the processor that created the chunk believes its gist to be, and this will influence whether or not a processor that receives the chunk will pay attention to it.

1.7 Comparing CTM w/GWTM
- For simplicity, have removed/simplified many features in GWT.
- Ex. CTM has just one "actor" on stage holding just one chunk at a time.
- In the CTM, all processors are in the LTM.
- We have eliminated the Central Executive, bc all of its actions can just come from the processors.
- In the CTM, inputs & outputs go directly to and from LTM processors, not directly thru STM.

2. The Feeling of Consciousness
- The feeling of consciousness is due principally to Brainish + architecture + special processors + predictive dynamics (cycles of prediction, feedback, learning)
1. Brainish: describes the sensory world as it is perceived
2. Architecture: Up-Tree comp + Down-Tree broadcast of the winner
3. Special processors: Here are a few processors with specialized algorithms built in at birth:
a. Model-of-the-World: Constructs models of CTM's worlds. Has direct output maps to CTM actuators.
- Has an inner world (approximate map of itself)
- Has an outer world (map it creates of the current environment)
- MotW processor tags parts of its models of the world (inner + outer) with labels and descriptions annotated in Brainish gists with sensations they can have and actions they can perform
b. Inner Speech: extracts whatever speech is encoded in the gist broadcast by STM, sends it to the same locations that the Input map sends gists of outer speech
c. Inner Vision, hearing, sensation: map STM broadcasts to whatever locations the Input maps send outer scenes and outer sensations...
4. Predictive dynamics: the continuous cycling through prediction, feedback, and learning + the stream of consciousness play a role in CTMs feelings of consciousness
5. A minimal ability to think and make plans
6. Motivation (= energy + drive) to make a plan and pursue it
