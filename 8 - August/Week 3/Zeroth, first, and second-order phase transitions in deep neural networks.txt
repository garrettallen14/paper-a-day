Zeroth, first, and second-order phase transitions in deep neural networks

14 Dec 2023

Could we say ... free energy loss is a form of regularization, and do without layernorm ??? 


Abstract
    - We investigate deep-learning-unique first-order and second-order phase transitions, whose phenomenology closely follows that in statistical physics
    - We prove that the competition between prediction error and model complexity in the training loss leads to:
        - Second-order phase transition for deep linear nets with one hidden layer 
        - First-order phase transition for nets with more than one hidden layer
    
    - We prove the linear origin theorem, which states that common deep nonlinear models are equivalent to a linear network of the same depth and connection structure close to the origin...
        - Therefore, the proposed theory is directly relevant to understandingt the optimization and initialization of neural networks and serves as a minimal model of the ubiquitous collapse phenomenon in DL


Introduction
    - In DL, learning proceeds as the parameters of different layers become structured so the model outputs correlate meaningfully to inputs
        - Reminiscent of an ordered phase in Physics, where microscoping degrees of freedom respond to external pertubations collectively
    - Regularization prevents model overfitting by limiting the correlation between model output and input, like an entropic force in physics that leads to disorder

    - We expect a phase transition from the regime where the regularization is negligible to a regime where it is dominant!

    - A series of recent works studied the under-to-over param phase transition in the context of linear regression.

    - We study the loss landscape of a DNN and prove that there exist phase transitions that can be described as the first- and second-order phase transitions with a striking similarity to statistical physics
    - We argue that these phase transitions can have profound implications for DL, such as the role of symmetry breaking, the qualitative distinction between shallow and deep architectures, and why collapses occur

    - For a multilayer linear net with stochastic neurons and trained with L2 regularization:
        1. We identify an order parameter and effective landscape that describes the phase transition between a trivial phase and a feature learning phase...
        2. We prove that:
            a. depth-0 nets (linear regression) have no phase transition
            b. depth-1 nets have the second-order phase transitions
            c. depth-D nets have the first-order phase transition for D > 1, and
            d. infinite-depth nets have the zeroth-order phase transition
        3. We prove that such networks approximate commonly used nonlinear networks of the same depth and connectivity structure

    - Our result implies that one can precisely classify the landscape of DNNs according to the Ehrenfest classification of phase transitions.
    - We discuss the relevance of the theory towards understanding optimization and initializtion of neural networks


Formal Framework
    - l(w,lam) a difftble loss function that is dependent on model param w and hyperparam lam
    - l can be decomposed into a data-dependent feature-learning term l0 and a data-independent term lamR(w) that regularizes the model at strength lam:
        - l(w, lam) = Ex[l0(w,x)] + lamR(w)
    
    - Learning amounts to finding the global minimizer of the loss function:
        - L(lam) := minw l(w,lam)
        - w* := argminw l(w, lam)

    - When lamda is large, lambdaR(w) causes w to stay close to zero
    - If L changes drastically against a small variation of lambda, it is hard to tune lamda to optimize the model performance
        - Thus, that L(lambda) is well-behaved is equivalent to lambda being an esay-to-tune hyperparameter...

    - We want to find the case where the tuning of lambda is difficult, which occurs where a phase transition comes into play
        - This formalism can be seen as a zero-temperature theory that ignores the stochastic effects due to the noise in the stochastic gradient Langevin dynamics

    - When the training proceeds with gradient flow and an injected isotropic Gaussian noise, the stationary distribution of the model params obeys the Gibbs distribution,

        p(w) proportional to exp[-l(w,lambda)/T]
        where T is proportional to the variance of the injected Gaussian noise in the gradient, and the partition function Z is given by the integral of exp[-l(w,lamda)/T] over w

    - The free energy is given by -TlogZ
        - In the limit T -> 0+, the partition function approaches the global minimizer of the loss function l:
            F(T = 0+, lambda) = -lim T-> 0+ TlogZ = L(lambda)

    - Therefore, lambda is ssomething like a nontemperature macroscopic thermodynamic variable, a little bit analogous to pressure!
        - This identification of the free energy is common in the replica-symmetry treatment of the learning of neural networks
    - In this view, optimization of the objective thus involves balancing the prediction error and the model complexity

    - We will be considering the phase transitions at zero temperature, so it is worthwhile to remark that in the Ehrenfest framework, a zero-temperature phase transition for a finite-size system is not forbidden
        - It is well-known that phase transitions can only happen when the system size tends to infinity, because e^E/T is analytic for any finite T and finite system!
        - However, functions of the Gibbs measure are not guaranteed to be analytic in the limit T -> 0, which will serve as the mathematical basis behind the finite-size zero-temperature phase transitions we study in this paper...

    - We define the order parameter and the effective loss as follows:
        Defn: b=b(w) in R is an order parameter of l(w,lam) if there exists a function l_ st for all lam, minw l_(b(w),lam) = L(lam), where l_ is said to be an effective loss function of l...
    - ie, an order parameter is a one-dimensional quantity whose minimization on l_ gives L(lam)

    - When lambda is large, lam R(w) causes w to stay close to zero
    - If L changes drastically against a small variation of lambda, it is hard to tune lambda to optimize the model performance
        - Thus, that L(lam) is well-behaved is equivalent to being an easy to tune hyperparameter

    - We are interested in the case where tuning of lam is difficult, which occurs where a phase transition comes into play (!!!)

    - This formalism can be seen as a zero-temperature theory that ignores the stochastic effects due to the noise in the stochastic gradient Langevin dynamics
        - When the training proceeds with gradient flow and an injected isotropic Gaussian noise (namely SGD), the stationary distribution of the model parameters obeys the Gibbs distribution
            p(w) proportional: exp(-l(w,lam)/T)
            where T is proportional to the variance of the injected Gaussian noise in the gradient, and the partition function Z is given by the integral of exp[-l(w,lam)/T] over w
            Free energy is given by -TlogZ
        - In the limit T -> 0+, the partition function approaches the global minimizer of the loss function l:
            F(T=0+, lam) = -lim T->0+ TlogZ = L(lam)
    
    - So, lambda is something like a nontemperature macroscopic thermodynamic variable, a little analogous to pressure

    - This identification of the FE is common in replica-symmetry treatment of learning of neural networks
    - In this view, optimization of the objective thus involves balancing the predicitno error and model complexity

    - Since we consider the phase transitions at zero temperature, it is worthwhile to remark that in the Ehrenfest framework, a zero-temperature phase transition for a finite-size system is not forbidden..
        - For a finite-temp system, phase transition can only happen when the system size tends to infinity
        - Gibbs measure exp(-E/T) is analytic for any finite T and finite system

    - Functions of the Gibbs measure (such as the free energy) are not guaranteeed to be analytic in the limit T->0, which will serve as the mathematical basiss behind the finite-size zero-temp phase transitions we study in this paper

    - We formally define the order parameter and the effective loss as follows:

    - The most general type of deep linear nets, with L2 regularization and stochastic neurons, has the following loss:
        .........

TABLE 1:
    - Norm of model (|b|) = Magnitude of order parameter
    - Feature learning regime = Ordered phase
    - Trivial regime = Disordered phase
    - Noise required for learning = Latent heat

    - b = 0 is a special point of the effective loss
    - We are interested in the case when b = 0 is the global minimum of the landscape and how it makes a transition away from b = 0
    - The two phases also have a clear meaning in the context of ML:
        - b = 0 is a trivial phase where no learning happens
        - b > 0 is the nontrivial phase where learning should occur for the model to reach the globabl minimum

    - We provide a perturbative analysis for better understanding:
        - When lam is large, one can expand the loss function around the origin:
        - l_ proportions to ,..................


Phase Transitions
No-phase-transition Theorems
    - It is worth discussing the case of D=0, which serves as a basis of comparison
    - The global minimum as a function of lambda is given by the ridge linear regression solution:
    - Theorem 1:
        - There is no phase transition in any hyperparam (lam, A0, E[xy], E[y**2]) in a simple ridge linear regression for any lambda in (0, inf)
    - We also prove a theorem showing that a finite-depth net cannot have zeroth-order phase transitions
        - This theorem allows the weight decay parameters